{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación binaria: 3 neuronas con varias columnas en la X\n",
    " \n",
    "La arquitectura que usaremos será:\n",
    "\n",
    "- **Capa oculta** de **2 neuronas**, cada una con activación ReLU.  \n",
    "  - Neurona 1: $z_1 = \\mathbf{w_1}^\\top \\mathbf{x} + b_1,\\; a_1 = \\mathrm{ReLU}(z_1)$.  \n",
    "  - Neurona 2: $z_2 = \\mathbf{w_2}^\\top \\mathbf{x} + b_2,\\; a_2 = \\mathrm{ReLU}(z_2)$.  \n",
    "  - $\\mathbf{x}$ tiene dimensión 2 (colesterol, glucosa). Por lo tanto, $\\mathbf{w_1}$ y $\\mathbf{w_2}$ son cada uno un vector de longitud 2.\n",
    "\n",
    "- **Capa de salida** (1 neurona) con activación sigmoide para predecir la **probabilidad** de clase 1 (apto).  \n",
    "  - Entrada: $(a_1, a_2)$.  \n",
    "  - Parámetros: $\\mathbf{w_3}\\in \\mathbb{R}^2$ y $b_3\\in \\mathbb{R}$.  \n",
    "  - $z_3 = w_{3,1} a_1 + w_{3,2} a_2 + b_3$.  \n",
    "  - $y_{\\text{pred}} = \\sigma(z_3)$.  \n",
    "\n",
    "La salida será un escalar entre 0 y 1, interpretado como la **probabilidad** de $y=1$. Usaremos **entropía cruzada binaria** (BCE) como función de costo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "Imaginemos un dataset (ficticio) con dos columnas: `[colesterol, glucosa]`, y la etiqueta binaria `[0,1]` según si es “no apto” o “apto”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X de forma (N, 2)\n",
    "# columnas: [colesterol, glucosa]\n",
    "X = np.array([\n",
    "    [180, 85],\n",
    "    [200, 90],\n",
    "    [220, 120],\n",
    "    [250, 130],\n",
    "    [270, 150],\n",
    "    [300, 180]\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([0, 0, 0, 1, 1, 1], dtype=float)\n",
    "\n",
    "N = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "Calcula la **salida de cada neurona** en orden.  \n",
    "1. **Neurona 1**: $z_1, a_1$  \n",
    "2. **Neurona 2**: $z_2, a_2$  \n",
    "3. **Neurona de salida**: $z_3, y_{\\text{pred}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def forward(X, w1, b1, w2, b2, w3, b3):\n",
    "    \"\"\"\n",
    "    X: (N,2)\n",
    "    w1, w2: cada uno (2,)\n",
    "    b1, b2: escalares\n",
    "    w3: (2,)  (entradas: a1, a2)\n",
    "    b3: escalar\n",
    "\n",
    "    Returns:\n",
    "      y_pred: shape (N,)  -- prob de ser clase 1\n",
    "      z1, a1, z2, a2, z3  (intermedios) para backprop\n",
    "    \"\"\"\n",
    "    # Neurona 1\n",
    "    z1 = np.dot(X, w1) + b1  # (N,)\n",
    "    a1 = relu(z1)            # (N,)\n",
    "\n",
    "    # Neurona 2\n",
    "    z2 = np.dot(X, w2) + b2  # (N,)\n",
    "    a2 = relu(z2)            # (N,)\n",
    "\n",
    "    # Neurona de salida\n",
    "    # Entrada = [a1, a2],   w3 = [w3[0], w3[1]]\n",
    "    # z3 = w3[0]*a1 + w3[1]*a2 + b3\n",
    "    # vectorizado:\n",
    "    z3 = w3[0]*a1 + w3[1]*a2 + b3  # (N,)\n",
    "\n",
    "    y_pred = sigmoid(z3)          # (N,)\n",
    "    return y_pred, z1, a1, z2, a2, z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función coste BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true)*np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientes\n",
    "\n",
    "Aplicaremos la **regla de la cadena** para derivar la BCE (binary cross entropy) respecto a cada parámetro: $w_1, b_1, w_2, b_2, w_3, b_3$.\n",
    "\n",
    "#### Derivadas para la neurona de salida\n",
    "\n",
    "Sea $z_3^{(i)} = w_3[0]\\cdot a_1^{(i)} + w_3[1]\\cdot a_2^{(i)} + b_3$, y $\\hat{y}^{(i)} = \\sigma(z_3^{(i)})$. La BCE:\n",
    "\n",
    "$$\n",
    "\\text{BCE} = -\\frac{1}{N}\\sum_{i=1}^N \\bigl[y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\bigr].\n",
    "$$\n",
    "\n",
    "La derivada $\\partial \\text{BCE}/\\partial z_3^{(i)}$ = $\\hat{y}^{(i)} - y^{(i)}$ (resultado estándar de la entropía cruzada con sigmoide).\n",
    "\n",
    "Por ende:\n",
    "\n",
    "1. $\\frac{\\partial}{\\partial w_3[0]} \\text{BCE}$ = $\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot a_1^{(i)}$.  \n",
    "2. $\\frac{\\partial}{\\partial w_3[1]} \\text{BCE}$ = $\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot a_2^{(i)}$.  \n",
    "3. $\\frac{\\partial}{\\partial b_3} \\text{BCE}$ = $\\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)})$.\n",
    "\n",
    "#### Derivadas para neuronas 1 y 2 (ocultas, con ReLU)\n",
    "\n",
    "Sea `d3` = $\\frac{\\partial \\text{BCE}}{\\partial z_3^{(i)}} = (\\hat{y}^{(i)} - y^{(i)})$.  \n",
    "\n",
    "Para la **Neurona 1** (con $z_1^{(i)} = w_1 \\cdot x^{(i)} + b_1$, $a_1^{(i)} = \\mathrm{ReLU}(z_1^{(i)})$):\n",
    "\n",
    "$$\n",
    "z_3^{(i)} \\text{ depende de } a_1^{(i)}, \\quad\n",
    "a_1^{(i)} = \\mathrm{ReLU}(z_1^{(i)})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial w_1} \n",
    "= \\sum_{i=1}^N \\bigl[\\frac{\\partial \\text{BCE}}{\\partial z_3^{(i)}} \\cdot \\frac{\\partial z_3^{(i)}}{\\partial a_1^{(i)}} \\cdot \\frac{\\partial a_1^{(i)}}{\\partial z_1^{(i)}} \\cdot \\frac{\\partial z_1^{(i)}}{\\partial w_1}\\bigr].\n",
    "$$\n",
    "\n",
    "- $\\frac{\\partial z_3^{(i)}}{\\partial a_1^{(i)}} = w_3[0]$.  \n",
    "- $\\frac{\\partial a_1^{(i)}}{\\partial z_1^{(i)}} = \\mathbf{1}_{z_1^{(i)} > 0}$ (derivada de ReLU: 1 si $z_1>0$, 0 si no).  \n",
    "- $\\frac{\\partial z_1^{(i)}}{\\partial w_1} = x^{(i)}$ (vector).  \n",
    "\n",
    "Por lo tanto, para cada ejemplo $i$:\n",
    "\n",
    "$$\n",
    "\\text{con } d3^{(i)} = (\\hat{y}^{(i)} - y^{(i)}),\n",
    "\\quad\n",
    "\\text{ReLU'}(z_1^{(i)}) = \\mathbf{1}_{z_1^{(i)} > 0},\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial w_1} \n",
    "= \\frac{1}{N}\\sum_{i=1}^N \\Bigl[d3^{(i)} \\cdot w_3[0] \\cdot \\mathbf{1}_{z_1^{(i)} > 0} \\cdot x^{(i)}\\Bigr].\n",
    "$$\n",
    "\n",
    "Análogamente, $\\frac{\\partial \\text{BCE}}{\\partial b_1}$ = $\\frac{1}{N}\\sum d3^{(i)} \\cdot w_3[0] \\cdot \\mathbf{1}_{z_1^{(i)}>0}$.\n",
    "\n",
    "Lo mismo para la **Neurona 2**, usando $w_3[1]$ y $\\mathbf{1}_{z_2^{(i)}>0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y, \n",
    "             w1, b1, w2, b2, w3, b3):\n",
    "    \"\"\"\n",
    "    Calcula gradientes de BCE wrt w1,b1, w2,b2, w3,b3.\n",
    "    Devuelve dw1, db1, dw2, db2, dw3, db3 (todos en forma NumPy).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    # 1. Forward para obtener valores intermedios\n",
    "    y_pred, z1, a1, z2, a2, z3 = forward(X, w1, b1, w2, b2, w3, b3)\n",
    "    \n",
    "    # 2. d3 = (y_pred - y), shape (N,)   (para cada ejemplo)\n",
    "    d3 = (y_pred - y)  # BCE con sigmoide -> deriv w.r.t z3\n",
    "    \n",
    "    # ================================\n",
    "    # Gradientes wrt w3, b3 (neurona de salida)\n",
    "    # w3 = [w3[0], w3[1]]\n",
    "    dw3_0 = np.mean(d3 * a1)  # partial wrt w3[0]\n",
    "    dw3_1 = np.mean(d3 * a2)  # partial wrt w3[1]\n",
    "    db3   = np.mean(d3)       # partial wrt b3\n",
    "    \n",
    "    dw3 = np.array([dw3_0, dw3_1])  # shape(2,)\n",
    "    \n",
    "    # ================================\n",
    "    # Gradientes wrt neurona 1 (z1 -> a1)\n",
    "    # d(z3)/da1 = w3[0]\n",
    "    # d(a1)/dz1 = ReLU'(z1) = 1_{z1>0}\n",
    "    relu_mask1 = (z1 > 0).astype(float)  # shape(N,)\n",
    "    da1_dz1 = relu_mask1\n",
    "    # Se multiplica todo: d3 * w3[0] * da1_dz1\n",
    "    dz1 = d3 * w3[0] * da1_dz1  # shape(N,)\n",
    "    # dw1 = mean( X.T * dz1 )\n",
    "    dw1 = np.dot(X.T, dz1) / N  # shape(2,)\n",
    "    # db1 = mean(dz1)\n",
    "    db1 = np.mean(dz1)\n",
    "    \n",
    "    # ================================\n",
    "    # Gradientes wrt neurona 2 (z2 -> a2)\n",
    "    relu_mask2 = (z2 > 0).astype(float)\n",
    "    dz2 = d3 * w3[1] * relu_mask2  # shape(N,)\n",
    "    dw2 = np.dot(X.T, dz2) / N     # shape(2,)\n",
    "    db2 = np.mean(dz2)\n",
    "    \n",
    "    return dw1, db1, dw2, db2, dw3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Ahora tenemos 6 **parámetros** en total:\n",
    "\n",
    "1. **Neurona 1 (capa oculta):**  \n",
    "   - $\\mathbf{w}_1$ (2,)  \n",
    "   - $b_1$ (escalar)\n",
    "2. **Neurona 2 (capa oculta):**  \n",
    "   - $\\mathbf{w}_2$ (2,)  \n",
    "   - $b_2$ (escalar)\n",
    "3. **Neurona de salida:**  \n",
    "   - $\\mathbf{w}_3$ (2,)  — pues entra $(a_1, a_2)$  \n",
    "   - $b_3$ (escalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/5000 | Loss=0.678275\n",
      "Epoch 2000/5000 | Loss=0.674815\n",
      "Epoch 3000/5000 | Loss=0.671053\n",
      "Epoch 4000/5000 | Loss=0.664765\n",
      "Epoch 5000/5000 | Loss=0.653674\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "w1 = np.random.randn(2) * 0.01\n",
    "b1 = 0.0\n",
    "\n",
    "w2 = np.random.randn(2) * 0.01\n",
    "b2 = 0.0\n",
    "\n",
    "w3 = np.random.randn(2) * 0.01\n",
    "b3 = 0.0\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward\n",
    "    y_pred, z1, a1, z2, a2, z3 = forward(X, w1, b1, w2, b2, w3, b3)\n",
    "    loss_value = binary_cross_entropy(y_pred, y)\n",
    "    \n",
    "    # Backward\n",
    "    dw1, db1, dw2, db2, dw3, db3 = backward(X, y, w1, b1, w2, b2, w3, b3)\n",
    "    \n",
    "    # Actualizar parámetros\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    w3 -= learning_rate * dw3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    # (Opcional) imprimir cada cierto número de iteraciones\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss={loss_value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenamiento terminado.\n",
      "w1 = [ 0.00585881 -0.00298174] b1 = 4.475841610629525e-05\n",
      "w2 = [-0.02455966  0.08116455] b2 = -0.0017523557559649093\n",
      "w3 = [-0.00470483  0.08322033] b3 = -0.03526826041500982\n",
      "Predicciones (prob): [0.54166742 0.53978358 0.5795923  0.5809423  0.60362339 0.63679952]\n",
      "Clases predichas:    [1 1 1 1 1 1]\n",
      "Clases reales:       [0. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntrenamiento terminado.\")\n",
    "print(\"w1 =\", w1, \"b1 =\", b1)\n",
    "print(\"w2 =\", w2, \"b2 =\", b2)\n",
    "print(\"w3 =\", w3, \"b3 =\", b3)\n",
    "\n",
    "y_pred_final, _, _, _, _, _ = forward(X, w1, b1, w2, b2, w3, b3)\n",
    "print(\"Predicciones (prob):\", y_pred_final)\n",
    "print(\"Clases predichas:   \", (y_pred_final >= 0.5).astype(int))\n",
    "print(\"Clases reales:      \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    y_pred_classes = (y_pred >= 0.5).astype(int)\n",
    "    correct_predictions = np.sum(y_true == y_pred_classes)\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "accuracy_value = accuracy_score(y, (y_pred_final >= 0.5).astype(int))\n",
    "print(f\"Accuracy: {accuracy_value:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
