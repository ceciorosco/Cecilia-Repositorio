{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación binaria: 1 neurona con varias columnas en X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "dataset ficticio para clasificación.\n",
    "\n",
    "Suponemos que tenemos dos variables de entrada:\n",
    "- $ x_1 $ = Colesterol,  \n",
    "- $ x_2 $ = Glucosa,\n",
    "\n",
    "y la salida será $ y \\in \\{0,1\\} $, donde 0 significa \"no apto\" y 1 significa \"apto\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X con forma (N, 2). Cada fila es [colesterol, glucosa].\n",
    "X = np.array([\n",
    "    [180, 85],\n",
    "    [200, 90],\n",
    "    [220, 120],\n",
    "    [250, 130],\n",
    "    [270, 150],\n",
    "    [300, 180]\n",
    "], dtype=float)\n",
    "\n",
    "# Etiquetas (0: no apto, 1: apto)\n",
    "y = np.array([0, 0, 1, 1, 1, 1], dtype=float)\n",
    "\n",
    "N = X.shape[0]  # Número de ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir neurona\n",
    "\n",
    "Matemáticamente, una **neurona** con **función sigmoide** (logistic) hace:\n",
    "\n",
    "$$\n",
    "z^{(i)} = w_1 \\cdot x_1^{(i)} + w_2 \\cdot x_2^{(i)} + b,\n",
    "$$\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma\\bigl(z^{(i)}\\bigr) = \\frac{1}{1 + e^{-\\,z^{(i)}}}.\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{w} = (w_1, w_2) $ son los pesos,  \n",
    "- $ b $ es el sesgo (bias),\n",
    "- $ \\hat{y}^{(i)} $ es la **probabilidad** de que el ejemplo $ i $ pertenezca a la clase 1 (apto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(X, w, b):\n",
    "    z = np.dot(X, w) + b  # (N,) = w1*X[:,0] + w2*X[:,1] + b\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo: Entropía Cruzada Binaria\n",
    "\n",
    "Para la **clasificación binaria**, la pérdida típica es la **binary cross-entropy** (BCE):\n",
    "$$\n",
    "\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^N \\Bigl[y^{(i)} \\log(\\hat{y}^{(i)}) + \\bigl(1 - y^{(i)}\\bigr)\\log\\bigl(1 - \\hat{y}^{(i)}\\bigr)\\Bigr].\n",
    "$$\n",
    "Este costo penaliza las predicciones que se alejan de la verdadera etiqueta (0 o 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    epsilon = 1e-7\n",
    "    # Clampeamos la predicción para evitar log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientes\n",
    "\n",
    "Con la neurona logística, es conocido que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial w_j}\n",
    "= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\, x_j^{(i)},\n",
    "$$\n",
    "\n",
    "para $ j = 1, 2 $. En forma **vectorizada**:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\text{BCE} \n",
    "= \\frac{1}{N} \\, X^\\top \\bigl(\\hat{y} - y\\bigr).\n",
    "$$\n",
    "\n",
    "Igualmente, respecto al sesgo $ b $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{BCE}}{\\partial b}\n",
    "= \\frac{1}{N} \\sum_{i=1}^N \n",
    "     (\\hat{y}^{(i)} - y^{(i)}).\n",
    "$$\n",
    "\n",
    "Estas derivadas permiten usar **descenso por gradiente** para actualizar $ w_1, w_2 $ y $ b $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Derivada de la BCE wrt w (tamaño (2,)).\n",
    "    \"\"\"\n",
    "    y_pred = forward(X, w, b)  # shape (N,)\n",
    "    # (y_pred - y) -> shape (N,)\n",
    "    # X.T -> shape (2, N)\n",
    "    # np.dot(X.T, y_pred - y) -> shape (2,)\n",
    "    return np.dot(X.T, (y_pred - y)) / len(X)\n",
    "\n",
    "def grad_b(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Derivada de la BCE wrt b (escalar).\n",
    "    \"\"\"\n",
    "    y_pred = forward(X, w, b)\n",
    "    return np.mean(y_pred - y)  # 1/N * sum(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Loss=0.002318, w=[-0.49469323  0.99119849], b=-0.0205\n",
      "Epoch 100/1000, Loss=0.002265, w=[-0.49699163  0.99579701], b=-0.0207\n",
      "Epoch 150/1000, Loss=0.002214, w=[-0.49923793  1.00029127], b=-0.0209\n",
      "Epoch 200/1000, Loss=0.002166, w=[-0.50143443  1.00468592], b=-0.0211\n",
      "Epoch 250/1000, Loss=0.002120, w=[-0.50358331  1.00898529], b=-0.0213\n",
      "Epoch 300/1000, Loss=0.002075, w=[-0.50568659  1.01319344], b=-0.0214\n",
      "Epoch 350/1000, Loss=0.002033, w=[-0.50774618  1.01731417], b=-0.0216\n",
      "Epoch 400/1000, Loss=0.001992, w=[-0.50976386  1.02135104], b=-0.0218\n",
      "Epoch 450/1000, Loss=0.001953, w=[-0.5117413   1.02530743], b=-0.0220\n",
      "Epoch 500/1000, Loss=0.001915, w=[-0.51368009  1.02918647], b=-0.0221\n",
      "Epoch 550/1000, Loss=0.001879, w=[-0.51558172  1.03299116], b=-0.0223\n",
      "Epoch 600/1000, Loss=0.001844, w=[-0.51744757  1.03672429], b=-0.0224\n",
      "Epoch 650/1000, Loss=0.001810, w=[-0.51927899  1.04038851], b=-0.0226\n",
      "Epoch 700/1000, Loss=0.001778, w=[-0.52107722  1.04398633], b=-0.0227\n",
      "Epoch 750/1000, Loss=0.001747, w=[-0.52284344  1.04752012], b=-0.0229\n",
      "Epoch 800/1000, Loss=0.001716, w=[-0.52457878  1.05099212], b=-0.0230\n",
      "Epoch 850/1000, Loss=0.001687, w=[-0.52628431  1.05440447], b=-0.0232\n",
      "Epoch 900/1000, Loss=0.001659, w=[-0.52796102  1.05775918], b=-0.0233\n",
      "Epoch 950/1000, Loss=0.001632, w=[-0.52960989  1.06105817], b=-0.0235\n",
      "Epoch 1000/1000, Loss=0.001605, w=[-0.53123183  1.06430328], b=-0.0236\n",
      "\n",
      "Entrenamiento terminado.\n",
      "Pesos finales (w): [-0.53123183  1.06430328]\n",
      "Sesgo final (b): -0.023614240568262966\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.randn(2) * 0.01  # inicialización aleatoria pequeña\n",
    "b = 0.0                        # sesgo en 0 para empezar (podríamos poner algo aleatorio también)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = forward(X, w, b)\n",
    "    loss_value = binary_cross_entropy(y_pred, y)\n",
    "    \n",
    "    dw = grad_w(X, y, w, b)\n",
    "    db = grad_b(X, y, w, b)\n",
    "    \n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # (opcional) cada cierto número de épocas\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss={loss_value:.6f}, w={w}, b={b:.4f}\")\n",
    "\n",
    "print(\"\\nEntrenamiento terminado.\")\n",
    "print(\"Pesos finales (w):\", w)\n",
    "print(\"Sesgo final (b):\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones (prob): [5.59893245e-03 2.80166144e-05 9.99980040e-01 9.96041228e-01\n",
      " 9.99999907e-01 1.00000000e+00]\n",
      "Clases predichas   : [0 0 1 1 1 1]\n",
      "Clases reales      : [0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = forward(X, w, b)\n",
    "print(\"Predicciones (prob):\", y_pred_final)\n",
    "print(\"Clases predichas   :\", (y_pred_final >= 0.5).astype(int))\n",
    "print(\"Clases reales      :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    # Convertir predicciones a clases binarias (0 o 1) si son probabilidades\n",
    "    y_pred_classes = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calcular el número de aciertos\n",
    "    correct_predictions = np.sum(y_true == y_pred_classes)\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "accuracy_value = accuracy_score(y, (y_pred_final >= 0.5).astype(int))\n",
    "print(f\"Accuracy: {accuracy_value:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
