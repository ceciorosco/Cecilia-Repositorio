{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión: 1 neurona con 1 sola columna en X\n",
    "\n",
    "\n",
    "## 1. Dataset con X, y\n",
    "\n",
    "Supongamos un problema de **regresión** muy sencillo:\n",
    "\n",
    "- **Entrada**: Años de experiencia ($ x $)  \n",
    "- **Salida**: Salario ($ y $)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=float)\n",
    "y = np.array([1500, 1700, 2000, 2200, 2600, 3000, 3200, 3500, 4000, 4500], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definir una neurona\n",
    "\n",
    "Cuando solo hay una neurona, el forward pass se refiere a calcular la salida de esa neurona a partir de sus entradas, pesos, y función de activación.\n",
    "\n",
    "Una neurona lineal simple (sin activación no lineal) se puede describir así:\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b,\n",
    "$$\n",
    "$$\n",
    "\\hat{y} = z,\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $x$ es tu dato de entrada (en este ejemplo, un solo valor: años de experiencia),\n",
    "- $w$ es el peso,\n",
    "- $b$ es el sesgo,\n",
    "- $\\hat{y}$ es la **predicción** de la neurona (el salario estimado).\n",
    "\n",
    "> Si fuese una neurona con función de activación, por ejemplo $ y = f(z) $, lo mencionaríamos, pero para un problema de regresión, habitualmente se usa la **identidad** (o sea, no se pone activación adicional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w, b):\n",
    "    \"\"\"\n",
    "    Hace la predicción lineal para todos los puntos X.\n",
    "    X: shape (N,) -> array de entradas\n",
    "    w: escalar (peso)\n",
    "    b: escalar (sesgo)\n",
    "    Return: vector de predicciones (shape (N,))\n",
    "    \"\"\"\n",
    "    # Z = w * X + b  (operación elemento a elemento)\n",
    "    return w * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de costo (MSE - Mean Squared Error)\n",
    "\n",
    "Si tenemos $ N $ ejemplos $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, definimos la **función de costo** (o pérdida) como el **error cuadrático medio**:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\bigl(\\hat{y}^{(i)} - y^{(i)}\\bigr)^{2}\n",
    "$$\n",
    "donde $\\hat{y}^{(i)} = w \\cdot x^{(i)} + b$.\n",
    "\n",
    "La función de costo mide la discrepancia entre las predicciones de la red $\\hat{y}$ y las etiquetas verdaderas $y$.\n",
    "\n",
    "**En código** (vectorizado con NumPy), si `X` es un array de tamaño $(N, )$ con todos los `x`, e `y` es un array de $(N, )$ con las etiquetas (salarios reales), podemos escribir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: predicciones (N,)\n",
    "    y_true: valores reales (N,)\n",
    "    \"\"\"\n",
    "    return np.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derivadas: cómo ajustamos w y b\n",
    "\n",
    "Una vez calculada la función de costo, queremos ajustar $\\mathbf{W}$ y $\\mathbf{b}$ para **minimizar** ese costo. \n",
    "\n",
    "Se utiliza el **Gradiente Descendente** y, para ello, necesitamos el **gradiente** de la función costo con respecto a cada parámetro $ w $ y $ b $.\n",
    "\n",
    "Para **entrenar** la neurona queremos **minimizar** el MSE. \n",
    "\n",
    "Esto se hace a través de **descenso por gradiente**, que usa las **derivadas** del MSE respecto a $ w $ y $ b $.\n",
    "\n",
    "**Una derivada** $\\frac{\\partial f}{\\partial w}$ te dice cómo cambiará $ f $ si cambias un poco $ w $ por ejemplo. \n",
    "\n",
    "En **redes neuronales**, necesitamos las derivadas de la **función de costo** respecto a todos los parámetros (pesos w y sesgos b) para *saber en qué dirección* y *cuánto* ajustar esos parámetros de forma que minimicemos el error.\n",
    "\n",
    "Usamos luego **descenso por gradiente** (o algoritmos más avanzados) para mover cada $ w, b $ en la dirección que *disminuye* el error.  \n",
    "\n",
    "### Derivada de MSE respecto a $ w $\n",
    "\n",
    "$$\n",
    "\\text{MSE}(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} (w \\cdot x^{(i)} + b - y^{(i)})^2.\n",
    "$$\n",
    "Tomando la derivada respecto a $ w $:\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w} \n",
    "= \\frac{2}{N} \\sum_{i=1}^{N} \\bigl(w \\cdot x^{(i)} + b - y^{(i)}\\bigr)\\cdot x^{(i)}.\n",
    "$$\n",
    "\n",
    "> Observa que $\\frac{\\partial}{\\partial w}(w \\cdot x^{(i)}) = x^{(i)}$.\n",
    "\n",
    "### Derivada de MSE respecto a $ b $\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial b} \n",
    "= \\frac{2}{N} \\sum_{i=1}^{N} \\bigl(w \\cdot x^{(i)} + b - y^{(i)}\\bigr)\\cdot 1.\n",
    "$$\n",
    "\n",
    "> Porque $\\frac{\\partial}{\\partial b}(b) = 1$.\n",
    "\n",
    "En la práctica, a menudo se ve una versión simplificada (sin el factor 2) si hemos definido MSE con $\\frac12$.  \n",
    "Pero aquí, como usamos `np.mean`, el factor 1/N ya está ahí, y el 2 se puede quedar o no, dependiendo de la convención. Lo importante es la consistencia.\n",
    "\n",
    "### En código\n",
    "\n",
    "En NumPy, podemos traducir estas sumas a operaciones de vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de MSE wrt w.\n",
    "    X, y: shape (N,)\n",
    "    w, b: escalares\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    # y_pred = w*X + b\n",
    "    y_pred = forward(X, w, b)\n",
    "    # (y_pred - y) -> shape (N,)\n",
    "    # Multiplicamos elemento a elemento por X -> shape (N,)\n",
    "    # Hacemos la media y multiplicamos por 2 (si seguimos la fórmula literal)\n",
    "    return (2.0 / N) * np.sum((y_pred - y) * X)\n",
    "\n",
    "def grad_b(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de MSE wrt b.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    y_pred = forward(X, w, b)\n",
    "    # (y_pred - y) -> shape (N,)\n",
    "    return (2.0 / N) * np.sum(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento (Descenso por gradiente)\n",
    "\n",
    "La optimización en este contexto es el proceso de encontrar los valores de los parámetros $ w $ y $ b $ que minimizan la función de pérdida. Esto se logra iterativamente mediante el algoritmo de **descenso por gradiente**, que utiliza la siguiente regla:\n",
    "\n",
    "$$\n",
    "w_{\\text{nuevo}} = w_{\\text{actual}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b_{\\text{nuevo}} = b_{\\text{actual}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "- **$\\eta$** es el *learning rate*.\n",
    "- **$\\frac{\\partial \\text{Loss}}{\\partial w}$** y **$\\frac{\\partial \\text{Loss}}{\\partial b}$** son los gradientes que indican en qué dirección ajustar los parámetros para reducir la pérdida.\n",
    "- Loss es la función costo, en este caso el MSE.\n",
    "\n",
    "Por tanto para **ajustar** $ w $ y $ b $, se sigue un esquema iterativo:\n",
    "\n",
    "1. **Inicializamos** $ w $ y $ b $ (por ejemplo, con valores aleatorios).\n",
    "2. **Calculamos** las derivadas $\\frac{\\partial \\text{MSE}}{\\partial w}$ y $\\frac{\\partial \\text{MSE}}{\\partial b}$ con los datos completos.\n",
    "3. **Actualizamos** $ w $ y $ b $ usando una tasa de aprendizaje $\\eta$:\n",
    "4. Repetimos durante un número de **epochs** (iteraciones) o hasta que el error sea suficientemente pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Loss: 140909.9149, w: 438.2242, b: 247.5753\n",
      "Epoch 100/1000, Loss: 96616.3909, w: 417.1706, b: 394.1470\n",
      "Epoch 150/1000, Loss: 67537.8563, w: 400.1120, b: 512.9060\n",
      "Epoch 200/1000, Loss: 48447.9061, w: 386.2903, b: 609.1298\n",
      "Epoch 250/1000, Loss: 35915.4244, w: 375.0914, b: 687.0947\n",
      "Epoch 300/1000, Loss: 27687.8965, w: 366.0176, b: 750.2653\n",
      "Epoch 350/1000, Loss: 22286.5550, w: 358.6655, b: 801.4489\n",
      "Epoch 400/1000, Loss: 18740.5942, w: 352.7086, b: 842.9201\n",
      "Epoch 450/1000, Loss: 16412.6841, w: 347.8820, b: 876.5220\n",
      "Epoch 500/1000, Loss: 14884.4197, w: 343.9713, b: 903.7477\n",
      "Epoch 550/1000, Loss: 13881.1198, w: 340.8026, b: 925.8072\n",
      "Epoch 600/1000, Loss: 13222.4572, w: 338.2353, b: 943.6808\n",
      "Epoch 650/1000, Loss: 12790.0476, w: 336.1551, b: 958.1628\n",
      "Epoch 700/1000, Loss: 12506.1723, w: 334.4696, b: 969.8967\n",
      "Epoch 750/1000, Loss: 12319.8092, w: 333.1039, b: 979.4041\n",
      "Epoch 800/1000, Loss: 12197.4625, w: 331.9974, b: 987.1074\n",
      "Epoch 850/1000, Loss: 12117.1424, w: 331.1009, b: 993.3489\n",
      "Epoch 900/1000, Loss: 12064.4125, w: 330.3745, b: 998.4061\n",
      "Epoch 950/1000, Loss: 12029.7955, w: 329.7859, b: 1002.5037\n",
      "Epoch 1000/1000, Loss: 12007.0696, w: 329.3090, b: 1005.8237\n",
      "\n",
      "Entrenamiento terminado.\n",
      "Parámetros finales: w = 329.3090186124373, b = 1005.8237014013431\n",
      "Predicciones finales: [1335.13272001 1664.44173863 1993.75075724 2323.05977585 2652.36879446\n",
      " 2981.67781308 3310.98683169 3640.2958503  3969.60486891 4298.91388753]\n",
      "Valores reales     : [1500. 1700. 2000. 2200. 2600. 3000. 3200. 3500. 4000. 4500.]\n"
     ]
    }
   ],
   "source": [
    "# 1. Inicializamos w y b en valores aleatorios pequeños\n",
    "np.random.seed(42)\n",
    "w = np.random.randn() * 0.01  # un valor aleatorio pequeño\n",
    "b = np.random.randn() * 0.01\n",
    "\n",
    "# 2. Configuración del entrenamiento\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "# 3. Ciclo de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # 1. Forward pass: calcular predicciones y pérdida\n",
    "    y_pred = forward(X, w, b)\n",
    "    loss_value = mse_loss(y_pred, y)\n",
    "    \n",
    "    # 2. Backward pass: calcular los gradientes\n",
    "    dw = grad_w(X, y, w, b)\n",
    "    db = grad_b(X, y, w, b)\n",
    "    \n",
    "    # 3. Actualizar parámetros weight y bias (descenso gradiente)\n",
    "    # Optimización de w y b para minimizar la función costo\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # (Opcional) Imprimir cada 50 iteraciones\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_value:.4f}, w: {w:.4f}, b: {b:.4f}\")\n",
    "\n",
    "print(\"\\nEntrenamiento terminado.\")\n",
    "print(f\"Parámetros finales: w = {w}, b = {b}\")\n",
    "y_pred_final = forward(X, w, b)\n",
    "print(\"Predicciones finales:\", y_pred_final)\n",
    "print(\"Valores reales     :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12006.705564050028)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss(y_pred_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(88.3189466910253)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mae_loss(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "mae_loss(y_pred_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9865936740017307)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2_score(y_pred, y_true):\n",
    "    # Suma total de cuadrados (variación de los datos respecto a la media)\n",
    "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    # Suma de los residuos al cuadrado (errores del modelo)\n",
    "    residual_variance = np.sum((y_true - y_pred) ** 2)\n",
    "    # R^2: 1 - (varianza explicada por el error / varianza total)\n",
    "    return 1 - (residual_variance / total_variance)\n",
    "\n",
    "r2_score(y_pred_final, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se extendería a red con más neuronas, capas y entradas X?\n",
    "\n",
    "1. **Más entradas**: Si $ x $ fuese un vector ($ x_1, x_2, \\dots, x_n $), la neurona sería\n",
    "   $$\n",
    "   z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b.\n",
    "   $$\n",
    "   Y en NumPy usaríamos `np.dot(w, X) + b`.\n",
    "   \n",
    "2. **Función de activación**: Si añades $ y = f(z) $ (sigmoide, ReLU, etc.), la derivada se multiplica por $ f'(z) $. Esto es la **regla de la cadena**.\n",
    "\n",
    "3. **Varias neuronas** (capas): Se compone cada salida como entrada de la siguiente capa. Para derivar, se aplica la regla de la cadena para cada capa. En NumPy se vectoriza para que todos los ejemplos se procesen en bloque (batch).\n",
    "\n",
    "Pero **los fundamentos** son estos: el cálculo de derivadas (slopes) de la función de costo respecto a tus parámetros es lo que te permite corregirlos y mejorar tu modelo.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
