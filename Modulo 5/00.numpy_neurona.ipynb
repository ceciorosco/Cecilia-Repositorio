{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurona artificial\n",
    "\n",
    "Una **neurona artificial** es un modelo matemático que imita el comportamiento básico de una neurona biológica. \n",
    "\n",
    "Su formulación consiste en tomar una combinación lineal de entradas, aplicar un sesgo (bias) y, posteriormente, transformar ese valor mediante una función de activación. \n",
    "\n",
    "\n",
    "### 1. **Fórmula básica con suma ponderada y función de activación**\n",
    "\n",
    "La forma más habitual es:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i\\, x_i + b\\right)\n",
    "$$\n",
    "\n",
    "**Donde:**\n",
    "\n",
    "- **$x_i$**:  \n",
    "  *Nombre:* Entrada (o característica) número $i$.  \n",
    "  *Significado:* Es el valor de la $i$-ésima característica que se suministra a la neurona. Por ejemplo, en procesamiento de imágenes, cada $x_i$ podría representar el valor de un píxel. En datasets tabulares puede ser una columna numérica como por ejemplo una edad.\n",
    "\n",
    "- **$w_i$**:  \n",
    "  *Nombre:* Peso asociado a la entrada $x_i$.  \n",
    "  *Significado:* Indica la importancia o contribución de la entrada $x_i$ a la salida final. Un peso alto amplifica la entrada, mientras que un peso bajo la atenúa. Sin los pesos, todas las entradas tendrían la misma importancia, lo que limitaría la capacidad de la red para aprender y distinguir entre diferentes patrones. Ajustando los pesos durante el entrenamiento, la red \"aprende\" qué características son más relevantes para resolver la tarea específica.\n",
    "\n",
    "- **$n$**:  \n",
    "  *Nombre:* Número total de entradas.  \n",
    "  *Significado:* Es la cantidad de señales de entrada que recibe la neurona.\n",
    "\n",
    "- **$b$**:  \n",
    "  *Nombre:* Sesgo (o bias).  \n",
    "  *Significado:* Es un término adicional que permite desplazar la función de activación. Sirve para ajustar la posición del límite de decisión y, en conjunto con los pesos, ayuda a que la neurona se adapte mejor a los datos. El sesgo actúa de manera similar al intercepto en una regresión lineal. Permite que la función de activación se desplace hacia la izquierda o derecha. Esto es crucial para que la neurona pueda ajustar su salida incluso cuando las entradas son cero o tienen valores pequeños.\n",
    "\n",
    "- **$f$**:  \n",
    "  *Nombre:* Función de activación.  \n",
    "  *Significado:* Es una función no lineal que transforma la suma ponderada de las entradas en la salida de la neurona. Entre las funciones más comunes se encuentran:\n",
    "  - **Función escalón (o función de Heaviside):**  \n",
    "    $$\n",
    "    f(z) =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{si } z \\ge 0 \\\\\n",
    "      0, & \\text{si } z < 0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "  - **Función sigmoide:**  \n",
    "    $$\n",
    "    f(z) = \\frac{1}{1+e^{-z}}\n",
    "    $$\n",
    "  - **ReLU (Rectified Linear Unit):**  \n",
    "    $$\n",
    "    f(z) = \\max(0, z)\n",
    "    $$\n",
    "  - Entre otras.\n",
    "\n",
    "- **$y$**:  \n",
    "  *Nombre:* Salida de la neurona.  \n",
    "  *Significado:* Es el valor resultante después de aplicar la función de activación a la suma ponderada de las entradas y el sesgo.\n",
    "\n",
    "\n",
    "### 2. **Representación vectorial**\n",
    "\n",
    "Usando notación de vectores, la fórmula se puede escribir de manera más compacta:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\mathbf{w}^\\top \\mathbf{x} + b\\right)\n",
    "$$\n",
    "\n",
    "**Donde:**\n",
    "\n",
    "- **$\\mathbf{x}$**:  \n",
    "  *Nombre:* Vector de entradas.  \n",
    "  *Significado:* $\\mathbf{x} = [x_1, x_2, \\dots, x_n]^\\top$ representa todas las señales de entrada de la neurona.\n",
    "\n",
    "- **$\\mathbf{w}$**:  \n",
    "  *Nombre:* Vector de pesos.  \n",
    "  *Significado:* $\\mathbf{w} = [w_1, w_2, \\dots, w_n]^\\top$ contiene los pesos asociados a cada entrada.\n",
    "\n",
    "- **$\\mathbf{w}^\\top \\mathbf{x}$**:  \n",
    "  *Nombre:* Producto escalar.  \n",
    "  *Significado:* Es la suma ponderada de las entradas, equivalente a $\\sum_{i=1}^{n} w_i\\, x_i$.\n",
    "\n",
    "- **$b$**, **$f$** y **$y$** conservan los mismos significados que anteriormente.\n",
    "\n",
    "\n",
    "\n",
    "### **Ejemplo con función escalón**\n",
    "\n",
    "Si se utiliza la función escalón para obtener una salida binaria (por ejemplo, en el caso del **Perceptrón**), la fórmula se escribe como:\n",
    "\n",
    "$$\n",
    "y =\n",
    "\\begin{cases}\n",
    "1, & \\text{si } \\sum_{i=1}^{n} w_i\\, x_i + b \\ge 0 \\\\\n",
    "0, & \\text{si } \\sum_{i=1}^{n} w_i\\, x_i + b < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Aquí, la función de activación $f$ es la función escalón, que devuelve 1 si la suma ponderada (más el sesgo) supera o iguala 0, y 0 en caso contrario.\n",
    "\n",
    "\n",
    "\n",
    "### **Ejemplo con función sigmoide**\n",
    "\n",
    "Para modelos que requieren una salida continua en el rango (0, 1), se suele usar la función sigmoide:\n",
    "\n",
    "$$\n",
    "y = \\sigma\\left(\\sum_{i=1}^{n} w_i\\, x_i + b\\right) = \\frac{1}{1 + e^{-\\left(\\sum_{i=1}^{n} w_i\\, x_i + b\\right)}}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- **$\\sigma(z)$**:  \n",
    "  *Nombre:* Función sigmoide.  \n",
    "  *Significado:* Transforma cualquier valor real $z$ en un valor entre 0 y 1, lo que es útil para modelar probabilidades o para redes neuronales en tareas de clasificación binaria.\n",
    "\n",
    "* Diferencia entre la función escalón y la función sigmoide:\n",
    "  * Escalón: \n",
    "    * Devuelve valores binarios discretos (por ejemplo, 0 o 1) de manera abrupta. Si la entrada es mayor o igual a un umbral (usualmente 0), devuelve 1; de lo contrario, devuelve 0. Esto implica que no ofrece una estimación de probabilidad, sino una decisión directa.\n",
    "    * Es discontinua en el punto de umbral y no tiene una transición suave entre los valores 0 y 1. Esto significa que pequeños cambios en la entrada alrededor del umbral pueden provocar cambios drásticos en la salida.\n",
    "    * No es diferenciable en el punto de umbral (y es constante en cada intervalo), lo que dificulta el uso de métodos de optimización basados en gradiente (como el descenso del gradiente) durante el entrenamiento de modelos.\n",
    "  * Sigmoide:\n",
    "    * Devuelve un valor continuo en el intervalo (0, 1), lo que permite interpretar la salida como la probabilidad de pertenecer a la clase positiva. Por ejemplo, una salida de 0.8 puede interpretarse como un 80% de probabilidad de que la muestra pertenezca a la clase 1.\n",
    "    * Es una función suave y continua. La transición gradual de 0 a 1 permite que pequeños cambios en la entrada se reflejen en pequeños cambios en la salida, lo que es útil para modelar incertidumbre y para interpretar la salida como una probabilidad.\n",
    "    * Es diferenciable en todos sus puntos, lo que permite calcular gradientes de manera eficiente. Esto es crucial para algoritmos como la retropropagación en redes neuronales, facilitando el ajuste de los parámetros durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2. , 2.5, 3. , 3.5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ejemplo de una neurona con una sola columna X y sin aplicar función de activación\n",
    "def neurona(X, w, b):\n",
    "     return w * X + b\n",
    " \n",
    "X = np.array([1, 2, 3, 4, 5])  # Una única característica para varias muestras.\n",
    "w = 0.5\n",
    "b = 1.0\n",
    "\n",
    "neurona(X, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81757448, 0.88079708, 0.92414182, 0.95257413, 0.97068777])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de una neurona con una sola columna X y aplicando función de activación sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def neurona(X, w, b):\n",
    "    z = w * X + b\n",
    "    return sigmoid(z) # ideal para clasificación binaria\n",
    "\n",
    "neurona(X, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo con múltiples columnas en la X sin función de activación\n",
    "def neurona(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "w = np.array([0.5, -0.5]) # al haber dos columnas creamos dos pesos weight\n",
    "# w = np.array([0.1, 0.2, -1.0, 4.0]) # ejemplo si tenemos 4 columnas, pueden ser tanto positivos como negativos\n",
    "b = 1.0\n",
    "neurona(X, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62245933, 0.62245933, 0.62245933])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo con múltiples columnas en la X con función de activación\n",
    "def neurona(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    return sigmoid(z)\n",
    "\n",
    "neurona(X, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En total siempre habrá un peso w para cada columna de la X.\n",
    "\n",
    "Por ejemplo si tenemos 10 columnas en la X, tendremos 10 pesos w, uno para cada columna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
