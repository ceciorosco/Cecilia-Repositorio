{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión: 2 neuronas con varias columnas en la X\n",
    "\n",
    "La estructura es la siguiente:\n",
    "\n",
    "1. **Capa oculta** (Neurona 1):\n",
    "   - Entrada: $\\mathbf{x}\\in\\mathbb{R}^2$ (dos características).\n",
    "   - Parámetros: $\\mathbf{w}_1 \\in \\mathbb{R}^2$ y $b_1 \\in \\mathbb{R}$.\n",
    "   - Salida lineal: $z_1 = \\mathbf{x}\\cdot\\mathbf{w}_1 + b_1$.\n",
    "   - Activación ReLU: $a_1 = \\max(0, z_1)$.\n",
    "\n",
    "2. **Capa de salida** (Neurona 2):\n",
    "   - Entrada: $a_1\\in\\mathbb{R}$ (escalar).\n",
    "   - Parámetros: $w_2 \\in \\mathbb{R}$ (un solo número) y $b_2 \\in \\mathbb{R}$.\n",
    "   - Salida lineal: $z_2 = a_1 \\cdot w_2 + b_2$.\n",
    "   - Sin activación adicional (la salida es $y_{\\text{pred}} = z_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X: shape (N, 2)\n",
    "# - 1ª columna: años de experiencia\n",
    "# - 2ª columna: nivel de estudios (codificado numéricamente)\n",
    "X = np.array([\n",
    "    [1, 0],   # persona1\n",
    "    [2, 0],   # persona2\n",
    "    [3, 1],   # persona3\n",
    "    [5, 1],   # persona4\n",
    "    [6, 2],   # persona5\n",
    "], dtype=float)\n",
    "\n",
    "# y: salarios (en miles, por ejemplo)\n",
    "y = np.array([20, 25, 35, 45, 60], dtype=float)\n",
    "\n",
    "N = X.shape[0]  # Número de ejemplos (N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "Función de activación ReLU:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z).\n",
    "$$\n",
    "\n",
    "1. **Capa oculta**:  \n",
    "   - $ z_1^{(i)} = X^{(i)} \\cdot w_1 + b_1 $  \n",
    "   - $ a_1^{(i)} = \\text{ReLU}(z_1^{(i)}) $\n",
    "\n",
    "2. **Capa de salida**:  \n",
    "   - $ z_2^{(i)} = a_1^{(i)} \\cdot w_2 + b_2 $  \n",
    "   - Predicción: $ y_{\\text{pred}}^{(i)} = z_2^{(i)} $\n",
    "\n",
    "> Devolvemos `z1` y `a1` porque los necesitaremos en el **backprop** (cálculo de gradientes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def forward(X, w1, b1, w2, b2):\n",
    "    # Capa oculta\n",
    "    z1 = np.dot(X, w1) + b1   # (N,)\n",
    "    a1 = relu(z1)            # (N,)  ReLU\n",
    "\n",
    "    # Capa de salida (lineal)\n",
    "    z2 = a1 * w2 + b2        # (N,)\n",
    "    y_pred = z2              # (N,)  (sin activación final)\n",
    "\n",
    "    return y_pred, z1, a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientes\n",
    "\n",
    "- `X`: entradas con dos columnas (**shape (N, 2)**).\n",
    "- `w1`: pesos de la primera capa (**shape (2,)**).\n",
    "- `b1`: sesgo de la primera capa (**escalar**).\n",
    "- `w2`: peso de la segunda capa (**escalar**).\n",
    "- `b2`: sesgo de la segunda capa (**escalar**).\n",
    "- `forward(X, w1, b1, w2, b2)`: función que realiza el forward pass.\n",
    "\n",
    "#### Cálculo del gradiente para `w2` y `b2` (segunda capa)\n",
    "\n",
    "- **`grad_w2`:**  \n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_2} = \\frac{2}{N} \\sum (y_{\\text{pred}} - y) \\cdot a_1\n",
    "$$\n",
    "\n",
    "- **`grad_b2`:**  \n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial b_2} = \\frac{2}{N} \\sum (y_{\\text{pred}} - y)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Cálculo del gradiente para `w1` y `b1` (primera capa)\n",
    "\n",
    "Aquí necesitamos aplicar la **regla de la cadena**. La derivada se ve afectada por la función ReLU, cuya derivada es:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}'(z) = \\begin{cases} \n",
    "1 & \\text{si } z > 0 \\\\\n",
    "0 & \\text{si } z \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "La derivada de la pérdida respecto a `w1` es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial w_1} = \\frac{2}{N} \\sum \\left( (y_{\\text{pred}} - y) \\cdot w_2 \\cdot \\text{ReLU}'(z_1) \\cdot X \\right)\n",
    "$$\n",
    "\n",
    "#### Explicación de los pasos\n",
    "\n",
    "1. **Forward pass**: Calcula `z1`, `a1` y `y_pred`.\n",
    "2. **Backward pass**:\n",
    "   - Para `w2` y `b2`, el gradiente se basa en el error directo entre `y_pred` y `y`.\n",
    "   - Para `w1` y `b1`, aplicamos la regla de la cadena. Multiplicamos el error por `w2` y la derivada de la función **ReLU** para propagar los gradientes hacia atrás.\n",
    "\n",
    "Por tanto:\n",
    "\n",
    "- **`grad_w2`** y **`grad_b2`** se calculan directamente a partir de la salida `a1`.\n",
    "- **`grad_w1`** y **`grad_b1`** usan la regla de la cadena, multiplicando el error por `w2` y la derivada de **ReLU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w2(y_pred, y, a1):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente respecto a w2 (escalar).\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    return (2.0 / N) * np.dot(a1, (y_pred - y))\n",
    "\n",
    "def grad_b2(y_pred, y):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente respecto a b2 (escalar).\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    return (2.0 / N) * np.sum(y_pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w1(X, y, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente respecto a w1 (vector de forma (2,)).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, w1) + b1        # shape (N,)\n",
    "    a1 = relu(z1)                  # shape (N,)\n",
    "    y_pred = a1 * w2 + b2          # shape (N,)\n",
    "\n",
    "    # Backward pass\n",
    "    error = (y_pred - y)           # shape (N,)\n",
    "    relu_derivative = (z1 > 0).astype(float)  # Derivada de ReLU: 1 donde z1 > 0, 0 en caso contrario\n",
    "\n",
    "    # Gradiente respecto a w1\n",
    "    return (2.0 / N) * np.dot(X.T, error * w2 * relu_derivative)\n",
    "\n",
    "def grad_b1(X, y, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente respecto a b1 (escalar).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, w1) + b1        # shape (N,)\n",
    "    a1 = relu(z1)                  # shape (N,)\n",
    "    y_pred = a1 * w2 + b2          # shape (N,)\n",
    "\n",
    "    # Backward pass\n",
    "    error = (y_pred - y)           # shape (N,)\n",
    "    relu_derivative = (z1 > 0).astype(float)  # Derivada de ReLU\n",
    "\n",
    "    # Gradiente respecto a b1\n",
    "    return (2.0 / N) * np.sum(error * w2 * relu_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa combinada en una sola función:\n",
    "\n",
    "def backward(X, y, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    Calcula los gradientes de la MSE wrt w1, b1, w2, b2.\n",
    "    Retorna dw1, db1, dw2, db2.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    \n",
    "    # 1. Forward (para obtener z1, a1, z2)\n",
    "    y_pred, z1, a1 = forward(X, w1, b1, w2, b2)\n",
    "    \n",
    "    # 2. r = y_pred - y  (residuo)\n",
    "    r = (y_pred - y)  # shape (N,)\n",
    "\n",
    "    # 3. Gradientes de la neurona de salida (z2)\n",
    "    # dw2 = (2/N)*sum((z2-y)*a1)\n",
    "    # db2 = (2/N)*sum((z2-y))\n",
    "    dw2 = (2.0 / N) * np.sum(r * a1)\n",
    "    db2 = (2.0 / N) * np.sum(r)\n",
    "    \n",
    "    # 4. Gradientes de la capa oculta (z1 -> a1 -> z2)\n",
    "    # dMSE/dz1 = (2.0/N)*r * w2 * ReLU'(z1)\n",
    "    # ReLU'(z1) = 1 si z1>0, 0 si z1<=0\n",
    "    relu_mask = (z1 > 0).astype(float)  # shape (N,)\n",
    "    dz1 = (2.0 / N) * r * w2 * relu_mask  # shape (N,)\n",
    "\n",
    "    # dw1 = sum(X.T * dz1)\n",
    "    #   x^{(i)} -> shape(2,)\n",
    "    #   dz1 -> shape(N,)\n",
    "    #   X: shape(N,2), X.T: shape(2,N)\n",
    "    dw1 = np.dot(X.T, dz1)  # shape(2,)\n",
    "    \n",
    "    # db1 = sum(dz1)  (derivada wrt b1)\n",
    "    db1 = np.sum(dz1)\n",
    "    \n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Al finalizar, tendremos valores de $\\mathbf{w}_1, b_1, w_2, b_2$ que ajustan la red a los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/5000 - Loss: 1.7296 w1:[1.48814629 1.56701076], b1:3.0149, w2:3.7072, b2:2.3781\n",
      "Epoch 2000/5000 - Loss: 1.3933 w1:[1.23778612 1.94325172], b1:3.1338, w2:3.8875, b2:2.4094\n",
      "Epoch 3000/5000 - Loss: 1.3889 w1:[1.21023368 1.98411757], b1:3.1458, w2:3.9091, b2:2.4125\n",
      "Epoch 4000/5000 - Loss: 1.3889 w1:[1.20757056 1.98806205], b1:3.1469, w2:3.9112, b2:2.4128\n",
      "Epoch 5000/5000 - Loss: 1.3889 w1:[1.20731668 1.98843803], b1:3.1470, w2:3.9114, b2:2.4128\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w1 = np.random.randn(2) * 0.01  # vector (2,)\n",
    "b1 = 0.0\n",
    "\n",
    "w2 = np.random.randn() * 0.01   # escalar\n",
    "b2 = 0.0\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # forward\n",
    "    y_pred, z1, a1 = forward(X, w1, b1, w2, b2)\n",
    "    loss_value = mse_loss(y_pred, y)\n",
    "    \n",
    "    # backward\n",
    "    dw1, db1, dw2, db2 = backward(X, y, w1, b1, w2, b2)\n",
    "    \n",
    "    # Actualizar parámetros\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    # (Opcional) Print each certain steps\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss_value:.4f}\",\n",
    "              f\"w1:{w1}, b1:{b1:.4f}, w2:{w2:.4f}, b2:{b2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenamiento finalizado.\n",
      "Parámetros: w1=[1.20731668 1.98843803], b1=3.1470202572665587, w2=3.911402265486647, b2=2.4128455209100226\n",
      "Predicciones finales: [19.44440888 24.16671007 36.66659227 46.11119465 58.61107685]\n",
      "Reales              : [20. 25. 35. 45. 60.]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEntrenamiento finalizado.\")\n",
    "print(f\"Parámetros: w1={w1}, b1={b1}, w2={w2}, b2={b2}\")\n",
    "\n",
    "y_pred_final, _, _ = forward(X, w1, b1, w2, b2)\n",
    "print(\"Predicciones finales:\", y_pred_final)\n",
    "print(\"Reales              :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción para X=[4,1.5]: [45.27768396]\n"
     ]
    }
   ],
   "source": [
    "X_nuevo = np.array([[4, 1.5]], dtype=float)\n",
    "y_nuevo_pred, _, _ = forward(X_nuevo, w1, b1, w2, b2)\n",
    "print(\"Predicción para X=[4,1.5]:\", y_nuevo_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
