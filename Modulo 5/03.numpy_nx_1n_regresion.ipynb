{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Regresión: 1 neurona con varias columnas en X\n",
    "\n",
    "\n",
    "## 1. Dataset con X, y\n",
    "\n",
    "Supongamos un problema de **regresión** muy sencillo:\n",
    "\n",
    "- **Entrada**: Años de experiencia y posición ($ x $)  \n",
    "- **Salida**: Salario ($ y $)\n",
    "\n",
    "La idea es muy similar al caso de una sola entrada, pero ahora tenemos un **vector de pesos** en lugar de un solo escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ejemplo de X con 2 columnas:\n",
    "#  - Primera columna: años de experiencia\n",
    "#  - Segunda columna: nivel de estudios (codificado numéricamente)\n",
    "X = np.array([\n",
    "    [1, 0],   # persona1: 1 año exp, nivel 0\n",
    "    [2, 0],   # persona2: 2 años exp, nivel 0\n",
    "    [3, 1],   # persona3: 3 años exp, nivel 1\n",
    "    [5, 1],   # persona4: 5 años exp, nivel 1\n",
    "    [6, 2],   # persona5: 6 años exp, nivel 2\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([20, 25, 35, 45, 60], dtype=float)  # salarios correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definir una neurona\n",
    "\n",
    "Imagina que para cada ejemplo $i$:\n",
    "\n",
    "- $ x_1^{(i)} $ = Años de experiencia  \n",
    "- $ x_2^{(i)} $ = Nivel de estudios (codificado como algún número entero, 0, 1, 2, etc.)\n",
    "\n",
    "El **modelo lineal** (la neurona) dice:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} \n",
    "= w_1 \\cdot x_1^{(i)} + w_2 \\cdot x_2^{(i)} + b,\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ w_1 $ y $ w_2 $ son los **pesos**,\n",
    "- $ b $ es el **sesgo** (bias),\n",
    "- $\\hat{y}^{(i)}$ es la **predicción** del salario para el ejemplo $i$.\n",
    "\n",
    "En forma vectorizada, si $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})$ y $\\mathbf{w} = (w_1, w_2)$, se escribe:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b.\n",
    "$$\n",
    "\n",
    "Calcula $ \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b $ para **todos** los ejemplos a la vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w, b):\n",
    "    \"\"\"\n",
    "    X: shape (N, 2)  (N filas, 2 columnas)\n",
    "    w: shape (2,)    \n",
    "    b: escalar\n",
    "    Return: y_pred de shape (N,)\n",
    "    \"\"\"\n",
    "    return np.dot(X, w) + b  # w[0]*X[:,0] + w[1]*X[:,1] + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de costo (MSE)\n",
    "\n",
    "Si tenemos $N$ ejemplos en total, definimos la **pérdida MSE** como:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\mathbf{w}, b) \n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} \\bigl(\\hat{y}^{(i)} - y^{(i)}\\bigr)^{2},\n",
    "$$\n",
    "donde $y^{(i)}$ es el salario real en el ejemplo $i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derivadas de w y b\n",
    "\n",
    "Para actualizar $\\mathbf{w}$ y $b$ con **descenso por gradiente**, necesitamos las **derivadas** de la MSE respecto a cada parámetro.\n",
    "\n",
    "1. **Gradiente respecto a $\\mathbf{w}$** (en forma vectorial):\n",
    "   $$\n",
    "   \\nabla_{\\mathbf{w}} \\text{MSE} \n",
    "   = \\frac{2}{N} \\sum_{i=1}^N \n",
    "     \\bigl(\\hat{y}^{(i)} - y^{(i)}\\bigr) \\, \\mathbf{x}^{(i)}.\n",
    "   $$\n",
    "   - Observa que $\\hat{y}^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b$.\n",
    "\n",
    "2. **Gradiente respecto a $b$**:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial b}\n",
    "   = \\frac{2}{N} \\sum_{i=1}^N \n",
    "     \\bigl(\\hat{y}^{(i)} - y^{(i)}\\bigr).\n",
    "   $$\n",
    "\n",
    "En forma **vectorizada**:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\text{MSE}(\\mathbf{w}, b)\n",
    "= \\frac{2}{N} \\, X^\\top \\bigl(\\hat{y} - y\\bigr).\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial b}\n",
    "= \\frac{2}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Retorna la derivada (gradiente) respecto a w, que será un vector (2,).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    y_pred = forward(X, w, b)            # shape (N,)\n",
    "    # (y_pred - y) -> shape (N,)\n",
    "    # np.dot(X.T, (y_pred - y)) -> shape (2,) \n",
    "    # factor 2.0/N por la derivada del cuadrado y la media\n",
    "    return (2.0 / N) * np.dot(X.T, (y_pred - y))\n",
    "\n",
    "def grad_b(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Retorna la derivada respecto a b (un escalar).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    y_pred = forward(X, w, b)\n",
    "    return (2.0 / N) * np.sum(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Elegimos una **tasa de aprendizaje** (`learning_rate`), un número de **épocas** y repetimos:\n",
    "\n",
    "1. Computar `y_pred`  \n",
    "2. Calcular pérdida (MSE)  \n",
    "3. Calcular `dw` y `db`  \n",
    "4. Actualizar `w` y `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500 - Loss: 13.5768, w: [8.46925079 1.53604604], b: 5.6097\n",
      "Epoch 200/500 - Loss: 8.4245, w: [8.00461021 1.4095659 ], b: 7.7951\n",
      "Epoch 300/500 - Loss: 6.1775, w: [7.6164443  1.65838487], b: 9.2054\n",
      "Epoch 400/500 - Loss: 4.9513, w: [7.28474243 2.07916559], b: 10.1665\n",
      "Epoch 500/500 - Loss: 4.1463, w: [6.99693182 2.56376392], b: 10.8604\n",
      "\n",
      "Entrenamiento terminado.\n",
      "w final = [6.99693182 2.56376392], b final = 10.860388678533837\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.randn(2) * 0.01  # vector de 2 pesos pequeños\n",
    "b = 0.0                        # iniciamos el sesgo en 0 (o un valor pequeño aleatorio)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Forward\n",
    "    y_pred = forward(X, w, b)\n",
    "    \n",
    "    # 2. Calcular costo (MSE)\n",
    "    loss_value = mse_loss(y_pred, y)\n",
    "    \n",
    "    # 3. Gradientes\n",
    "    dw = grad_w(X, y, w, b)  # vector shape (2,)\n",
    "    db = grad_b(X, y, w, b)  # escalar\n",
    "    \n",
    "    # 4. Actualizar parámetros\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # (Opcional) mostramos cada cierto número de épocas\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss_value:.4f}, w: {w}, b: {b:.4f}\")\n",
    "\n",
    "# Resultados finales\n",
    "print(\"\\nEntrenamiento terminado.\")\n",
    "print(f\"w final = {w}, b final = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones finales: [17.8573205  24.85425232 34.41494806 48.4088117  57.96950744]\n",
      "Valores reales      : [20. 25. 35. 45. 60.]\n"
     ]
    }
   ],
   "source": [
    "# Predicción final\n",
    "\n",
    "y_pred_final = forward(X, w, b)\n",
    "print(\"Predicciones finales:\", y_pred_final)\n",
    "print(\"Valores reales      :\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.662556675631975)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mae_loss(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "\n",
    "mae_loss(y_pred_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9799053389864572)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def r2_score(y_pred, y_true):\n",
    "    # Suma total de cuadrados (variación de los datos respecto a la media)\n",
    "    total_variance = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    # Suma de los residuos al cuadrado (errores del modelo)\n",
    "    residual_variance = np.sum((y_true - y_pred) ** 2)\n",
    "    # R^2: 1 - (varianza explicada por el error / varianza total)\n",
    "    return 1 - (residual_variance / total_variance)\n",
    "\n",
    "r2_score(y_pred_final, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción para X=[4, 1.5]: [42.69376184]\n"
     ]
    }
   ],
   "source": [
    "X_nuevo = np.array([\n",
    "    [4, 1.5]\n",
    "], dtype=float)  # 1 sola fila, 2 columnas\n",
    "\n",
    "y_nuevo_pred = forward(X_nuevo, w, b)\n",
    "print(\"Predicción para X=[4, 1.5]:\", y_nuevo_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
