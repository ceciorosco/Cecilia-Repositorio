{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning en Scikit Learn\n",
    "\n",
    "Scikit Learn se especializa más en el proceso de ciencia de datos y machine learning, incorpora clases para aprendizaje profundo (deep learning) pero no es una librería especializada en aprendizaje profundo o deep learning.\n",
    "\n",
    "* Perceptron: modelo más básico de una neurona artificial. Utilizado para clasificación binaria.\n",
    "* MLPClassifier: Red neuronal para clasificación.\n",
    "* MLPRegressor: Red neuronal para regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "El **Perceptrón** es el modelo más básico de neurona artificial, introducido por Frank Rosenblatt en 1958.\n",
    "\n",
    "Se limita a tomar una combinación lineal de entradas, aplicarle un umbral (threshold) y devolver una salida binaria. **No admite funciones de activación no lineales**, lo que limita su capacidad de resolver problemas complejos.\n",
    "\n",
    "- **Estructura**: Consta de una sola neurona (una única capa de pesos) que recibe entradas $x_1, x_2, \\ldots, x_n$ y produce una salida binaria (generalmente $\\{0,1\\}$ o $\\{-1,1\\}$) usando una función de activación tipo umbral (step function).  \n",
    "\n",
    "- **Aprendizaje**: El aprendizaje del perceptrón consiste en ajustar los pesos de la neurona para separar correctamente los datos en dos clases (linealmente separables).  \n",
    "\n",
    "- **Limitaciones**: El perceptrón estándar solo puede aprender fronteras lineales. Si el conjunto de datos no es linealmente separable, el algoritmo no convergerá usando la regla de actualización clásica.  \n",
    "\n",
    "Básicamente es una función matemática que recibe una entrada y da una salida.\n",
    "\n",
    "La entrada son las features (X) a las cuales se puede asignar un peso (weight).\n",
    "\n",
    "Fórmula del perceptrón:\n",
    "\n",
    "1. **Entradas y pesos:**  \n",
    "\n",
    "Se parte de un conjunto de características o entradas representadas por un vector \"X\"  \n",
    "$$\n",
    "\\mathbf{x} = (x_1, x_2, \\dots, x_n)\n",
    "$$  \n",
    "y cada entrada tiene un peso asociado, conformando el vector de pesos  \n",
    "$$\n",
    "\\mathbf{w} = (w_1, w_2, \\dots, w_n).\n",
    "$$\n",
    "\n",
    "2. **Cálculo de la suma ponderada:**  \n",
    "Se multiplica cada entrada por su peso y se suman todos los resultados:\n",
    "$$\n",
    "S = w_1x_1 + w_2x_2 + \\dots + w_nx_n = \\sum_{i=1}^{n} w_i x_i.\n",
    "$$\n",
    "\n",
    "3. **Incorporar el sesgo (bias):**  \n",
    "Al resultado anterior se le añade un valor adicional llamado sesgo $ b $, que permite ajustar el umbral de activación:\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b.\n",
    "$$\n",
    "\n",
    "4. **Aplicar la función de activación:**  \n",
    "Finalmente, se pasa el valor $ z $ a través de una función de activación $ f(z) $ para determinar la salida del perceptrón.  \n",
    "\n",
    "En un perceptrón clásico se utiliza la función escalón:\n",
    "\n",
    "$$\n",
    "f(z) =\n",
    "\\begin{cases}\n",
    "1 & \\text{si } z \\ge 0, \\\\\n",
    "0 & \\text{si } z < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "Por lo tanto, la salida final es:\n",
    "$$\n",
    "y = f(z).\n",
    "$$\n",
    "\n",
    "Parámetros en scikit:\n",
    "\n",
    "* ``max_iter``: Es el número máximo de epochs o iteraciones sobre el conjunto de datos que el algoritmo realizará durante el entrenamiento.\n",
    "* ``tol``: Es la tolerancia o umbral mínimo de cambio en el error (o en la función de costo) entre iteraciones para considerar que el algoritmo ha convergido.\n",
    "* ``penalty`` y ``alpha``:\n",
    "    * ``penalty``: Define el tipo de regularización a aplicar (por ejemplo, 'l2', 'l1', o 'elasticnet').\n",
    "    * ``alpha``: Es el coeficiente que multiplica el término de regularización, controlando su fuerza.\n",
    "    * Estas opciones ayudan a controlar la complejidad del modelo, evitando que se ajuste demasiado a los datos de entrenamiento (overfitting).\n",
    "* ``eta0`` y ``learning_rate``:\n",
    "    * ``eta0``: Es la tasa de aprendizaje inicial, que determina el tamaño de los pasos con los que se actualizan los pesos durante cada iteración.\n",
    "    * ``learning_rate``: Define la estrategia para ajustar la tasa de aprendizaje a lo largo del entrenamiento (por ejemplo, \"constant\", \"optimal\", \"invscaling\").\n",
    "    * Una tasa de aprendizaje demasiado alta puede hacer que el algoritmo oscile o incluso diverja, saltándose la solución óptima.\n",
    "    * Una tasa muy baja puede ralentizar el proceso de convergencia, haciendo que el entrenamiento sea ineficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# para que sea clasificación multiclase filtramos solo clases 0 y 1\n",
    "X, y = X[y < 2], y[y < 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy en train 1.0\n",
      "accuracy en test 1.0\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(max_iter=50, random_state=42) # por defecto 1000 epochs\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('accuracy en train', model.score(X_train, y_train))\n",
    "print('accuracy en test', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal clasificación: MLPClassifier\n",
    "\n",
    "Los **MLP (Multi-Layer Perceptrons)** son redes neuronales feedforward con una o varias capas ocultas, que suelen utilizar activaciones no lineales (ReLU, tanh, logistic, etc.). \n",
    "\n",
    "- **Arquitectura**: Múltiples capas densas (fully-connected). \n",
    "- **Aprendizaje**: \n",
    "  - Se optimiza una función de coste adecuada para clasificación (generalmente entropía cruzada).\n",
    "  - Incluye diferentes funciones de activación (relu, logistic, tanh, identity).\n",
    "  - Diversos *solvers* para realizar la optimización (adam, sgd, lbfgs, etc.).  \n",
    "\n",
    "El objetivo es realizar predicciones de clase o probabilidades de clase cuando se usan funciones como la softmax en la última capa.\n",
    "\n",
    "- `hidden_layer_sizes`: tupla que indica el número de neuronas en cada capa oculta. Por ejemplo, `(100,)` sería una sola capa con 100 neuronas; `(100, 50)` serían dos capas ocultas, la primera de 100 neuronas y la segunda de 50.\n",
    "- `activation`: función de activación de las capas ocultas (por defecto `'relu'`, pero también `'logistic'`, `'tanh'`, `'identity'`).\n",
    "- `solver`: optimizador que se usará, puede ser `'adam'`, `'sgd'` o `'lbfgs'`.\n",
    "- `alpha`: parámetro de regularización L2.\n",
    "- `batch_size`: tamaño del minibatch (si se usa `'sgd'` o `'adam'`).\n",
    "- `learning_rate`: puede ser `'constant'`, `'invscaling'` o `'adaptive'`.  \n",
    "- `max_iter`: número máximo de iteraciones (épocas).\n",
    "- `shuffle`: si se barajan los datos en cada iteración.\n",
    "- `early_stopping`: si se activa, separa automáticamente un 10% de los datos de entrenamiento para validación y detiene el entrenamiento cuando no mejora.\n",
    "- `random_state`: semilla para reproducibilidad.\n",
    "- `tol`: tolerancia para la parada anticipada.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy en train 1.0\n",
      "accuracy en test 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# clasificación binaria:\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X, y = X[y < 2], y[y < 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(2), max_iter=2000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('accuracy en train', model.score(X_train, y_train))\n",
    "print('accuracy en test', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy en train 0.9732142857142857\n",
      "accuracy en test 1.0\n"
     ]
    }
   ],
   "source": [
    "# clasificación multiclase:\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes=(50), max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('accuracy en train', model.score(X_train, y_train))\n",
    "print('accuracy en test', model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
