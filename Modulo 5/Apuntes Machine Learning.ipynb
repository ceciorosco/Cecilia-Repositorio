{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRERIAS MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Librerías para Manejo y Procesamiento de Datos\n",
    "\n",
    "El manejo y la transformación de datos son pasos esenciales en cualquier proyecto de machine learning. Aquí se describen las librerías clave para esta tarea:\n",
    "\n",
    "### 1.1 Pandas\n",
    "\n",
    "**Descripción:**  \n",
    "Pandas es la librería más utilizada para la manipulación y análisis de datos en Python. Su estructura central, el *DataFrame*, permite trabajar de manera sencilla y flexible con datos tabulares.\n",
    "\n",
    "**Características:**\n",
    "- **Indexación y Selección:** Acceso a filas y columnas mediante etiquetas o posiciones.\n",
    "- **Manejo de Datos Faltantes:** Funciones para detectar, limpiar o imputar valores nulos.\n",
    "- **Operaciones de Agregación y Fusión:** Permite agrupar datos, calcular estadísticas y combinar diferentes datasets.\n",
    "- **Integración:** Compatible con librerías de visualización (como Matplotlib) y análisis numérico (como NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Cargar un dataset desde un archivo CSV\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Visualizar las primeras filas\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un dataset desde un archivo CSV\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Visualizar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "# Rellenar valores faltantes utilizando la propagación hacia adelante (forward fill)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Calcular la media de una columna\n",
    "media_columna = df['columna'].mean()\n",
    "print(\"Media de la columna:\", media_columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NumPy\n",
    "\n",
    "NumPy es la base para el cómputo numérico en Python, ofreciendo el objeto ndarray para almacenar y operar con arrays multidimensionales de forma eficiente.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "- Operaciones Vectorizadas: Realiza cálculos en arrays completos sin bucles explícitos, lo que mejora el rendimiento.\n",
    "- Funciones Matemáticas y Estadísticas: Incluye herramientas para álgebra lineal, transformadas de Fourier, generación de números aleatorios, entre otros.\n",
    "- Interoperabilidad: Muchas librerías científicas y de machine learning se construyen sobre NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array original:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "Array multiplicado por 2:\n",
      " [[ 2  4  6]\n",
      " [ 8 10 12]]\n",
      "Media: 3.5 Desviación Estándar: 1.707825127659933\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear un array 2D de 2x3\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Array original:\\n\", arr)\n",
    "\n",
    "# Multiplicar cada elemento del array por 2\n",
    "arr_doble = arr * 2\n",
    "print(\"Array multiplicado por 2:\\n\", arr_doble)\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "media = np.mean(arr)\n",
    "std_dev = np.std(arr)\n",
    "print(\"Media:\", media, \"Desviación Estándar:\", std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 SciPy\n",
    "\n",
    "SciPy amplía las capacidades de NumPy, proporcionando módulos especializados en cálculos científicos y matemáticos, tales como optimización, integración e interpolación.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "- Optimización e Integración: Herramientas para resolver ecuaciones y optimizar funciones.\n",
    "- Estadísticas y Procesamiento de Señales: Funciones avanzadas para análisis estadístico y manipulación de señales.\n",
    "- Álgebra Lineal: Funciones adicionales para resolver sistemas de ecuaciones y descomposiciones de matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de la integración: 9.000000000000002\n"
     ]
    }
   ],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "# Calcular la integral de la función f(x) = x^2 entre 0 y 3\n",
    "resultado, error = integrate.quad(lambda x: x**2, 0, 3)\n",
    "print(\"Resultado de la integración:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Librerías para Machine Learning\n",
    "Estas librerías implementan algoritmos y herramientas que permiten entrenar, validar y desplegar modelos de machine learning, desde métodos clásicos hasta técnicas de deep learning.\n",
    "\n",
    "### 2.1 Scikit-learn\n",
    "\n",
    "Scikit-learn es la librería más popular para machine learning clásico. Ofrece implementaciones para clasificación, regresión, clustering, reducción de dimensionalidad y preprocesamiento de datos.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Amplia Colección de Modelos: Incluye algoritmos como regresión lineal, SVM, árboles de decisión, entre otros.\n",
    "Preprocesamiento: Herramientas para normalizar, escalar y transformar datos.\n",
    "Validación y Evaluación: Funciones para validación cruzada, búsqueda de hiperparámetros y evaluación de modelos.\n",
    "Documentación y Comunidad: Abundante documentación y una comunidad activa que facilita el aprendizaje y la solución de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar un modelo RandomForest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones y evaluar el modelo\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 XGBoost\n",
    "\n",
    "XGBoost es una implementación optimizada de gradient boosting que se destaca por su eficiencia, escalabilidad y rendimiento en competiciones de machine learning.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Alto Rendimiento: Soporta paralelismo y optimizaciones específicas para grandes volúmenes de datos.\n",
    "Regularización: Incluye técnicas para evitar el sobreajuste.\n",
    "Flexibilidad: Puede utilizarse tanto para problemas de clasificación como de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convertir los datos en DMatrix, formato óptimo para XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir parámetros y entrenar el modelo\n",
    "params = {'objective': 'multi:softmax', 'num_class': 3, 'max_depth': 3, 'eta': 0.1}\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "predictions = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LightGBM\n",
    "\n",
    "Desarrollado por Microsoft, LightGBM es un framework de boosting que se destaca por su rapidez y eficiencia, especialmente en datasets de gran tamaño y con alta cardinalidad.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Rendimiento: Mejor utilización de memoria y velocidad de entrenamiento.\n",
    "Eficiencia en Datos Grandes: Diseñado para trabajar con grandes volúmenes de datos y altas dimensiones.\n",
    "Facilidad de Uso: Requiere menos ajustes de parámetros en comparación con otros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el Dataset para LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5\n",
    "}\n",
    "model = lgb.train(params, train_data, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TensorFlow/Keras\n",
    "\n",
    "TensorFlow es una plataforma de código abierto desarrollada por Google para construir y entrenar modelos de deep learning. Keras es una API de alto nivel que se integra con TensorFlow, facilitando la creación de redes neuronales de forma sencilla y rápida.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Flexibilidad: Permite construir modelos desde simples hasta muy complejos.\n",
    "Ecosistema Completo: Herramientas para visualización (TensorBoard), despliegue y optimización de modelos.\n",
    "Comunidad y Recursos: Amplia documentación, tutoriales y una comunidad activa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Definir un modelo secuencial simple para clasificación\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(4,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Cargar el dataset Iris e iniciar el entrenamiento\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "model.fit(data.data, data.target, epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 PyTorch\n",
    "\n",
    "PyTorch es un framework de deep learning desarrollado por Facebook, conocido por su capacidad para definir dinámicamente las redes (define-by-run) y por su facilidad para la investigación y el desarrollo de modelos personalizados.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Flexibilidad: Permite una gran libertad para diseñar y modificar modelos sobre la marcha.\n",
    "Interfaz Intuitiva: Diseño similar a Python que facilita su aprendizaje y uso.\n",
    "Comunidad en Crecimiento: Amplio soporte y numerosas implementaciones en investigación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir un modelo simple usando PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Aquí se incluiría el bucle de entrenamiento y evaluación con tus datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Librerías para Visualización\n",
    "La visualización es fundamental para el análisis exploratorio, la interpretación de resultados y la presentación de datos de manera clara.\n",
    "\n",
    "### 3.1 Matplotlib\n",
    "\n",
    "Matplotlib es la librería básica para crear gráficos estáticos, animados e interactivos en Python.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Altamente Personalizable: Permite ajustar casi cualquier aspecto del gráfico.\n",
    "Base para Otras Herramientas: Muchas otras librerías de visualización se basan en Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generar datos para graficar\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Crear un gráfico de la función seno\n",
    "plt.plot(x, y, label='sin(x)')\n",
    "plt.title(\"Gráfico del Seno de x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sin(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seaborn\n",
    "\n",
    "Seaborn es una librería construida sobre Matplotlib que facilita la creación de gráficos estadísticos con una estética moderna y atractiva.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Simplicidad: Funciones de alto nivel para crear gráficos complejos con pocas líneas de código.\n",
    "Integración con Pandas: Facilita el trabajo con DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(100),\n",
    "    'y': np.random.randn(100)\n",
    "})\n",
    "\n",
    "# Graficar un scatter plot utilizando Seaborn\n",
    "sns.scatterplot(x='x', y='y', data=df)\n",
    "plt.title(\"Scatter Plot con Seaborn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Plotly\n",
    "\n",
    "Plotly es una librería para crear visualizaciones interactivas y dashboards, ideal para análisis exploratorios y aplicaciones web.\n",
    "\n",
    "**Características:**\n",
    "\n",
    "Interactividad: Permite funciones de zoom, hover y selección en los gráficos.\n",
    "Facilidad de Integración: Se integra fácilmente en aplicaciones web y notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Usar un dataset de ejemplo que viene con Plotly\n",
    "df = px.data.iris()\n",
    "\n",
    "# Crear un gráfico interactivo\n",
    "fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\",\n",
    "                 title=\"Scatter Plot interactivo con Plotly\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANEJO Y PROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Librerías para el Manejo y Procesamiento de Datos\n",
    "\n",
    "El preprocesamiento y la manipulación de datos son pasos fundamentales en cualquier proyecto de machine learning. A continuación se detalla el ecosistema de librerías para trabajar con datos, desde operaciones básicas hasta procesamiento distribuido y en grandes volúmenes.\n",
    "\n",
    "### 1.1 Pandas\n",
    "\n",
    "**Descripción:**  \n",
    "Pandas es la librería por excelencia para el manejo de datos tabulares en . Su estructura principal, el *DataFrame*, permite trabajar de forma intuitiva con conjuntos de datos, facilitando la limpieza, transformación, agregación y análisis de información.\n",
    "\n",
    "**Características Principales:**  \n",
    "- **Indexación y Selección:** Acceso a datos mediante etiquetas, índices y condiciones.\n",
    "- **Manejo de Datos Faltantes:** Métodos para identificar, reemplazar o eliminar valores nulos.\n",
    "- **Integración:** Se integra de forma nativa con otras librerías como Matplotlib para visualización y SciPy para cálculos científicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un dataset desde un archivo CSV\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Mostrar las primeras 5 filas del DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Reemplazar datos faltantes utilizando el método 'ffill' (forward fill)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Agregar datos: calcular la media de una columna\n",
    "media_valor = df['columna'].mean()\n",
    "print(\"Media de 'columna':\", media_valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NumPy\n",
    "\n",
    "NumPy es la base para los cálculos numéricos en python. Proporciona el objeto ndarray, que es esencial para almacenar y manipular datos en forma de arrays multidimensionales, lo que permite realizar operaciones matemáticas de forma eficiente.\n",
    "\n",
    "**Características Principales:**\n",
    "\n",
    "- Operaciones Vectorizadas: Permiten aplicar operaciones aritméticas a arrays completos sin necesidad de bucles explícitos.\n",
    "- Álgebra Lineal y Estadística: Funciones integradas para álgebra lineal, transformadas de Fourier, generación de números aleatorios, entre otros.\n",
    "- Interoperabilidad: Funciona como backend para muchas otras librerías científicas y de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear un array 2D de 2x3\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Array original:\\n\", arr)\n",
    "\n",
    "# Realizar una operación vectorizada: multiplicar cada elemento por 2\n",
    "arr_doble = arr * 2\n",
    "print(\"Array multiplicado por 2:\\n\", arr_doble)\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "media = np.mean(arr)\n",
    "std_dev = np.std(arr)\n",
    "print(\"Media:\", media, \"Desviación Estándar:\", std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 SciPy\n",
    "\n",
    "SciPy extiende la funcionalidad de NumPy, ofreciendo módulos especializados en cálculos científicos y matemáticos avanzados, tales como optimización, integración, interpolación, procesamiento de señales y más.\n",
    "\n",
    "**Características Principales:**\n",
    "\n",
    "- Optimización e Integración: Herramientas para resolver ecuaciones y optimizar funciones.\n",
    "- Estadísticas y Álgebra Lineal: Múltiples funciones estadísticas y de análisis matricial.\n",
    "- Aplicaciones Especializadas: Funciones para procesamiento de señales, imágenes y resolución de problemas de física."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "import numpy as np\n",
    "\n",
    "# Calcular la integral de la función f(x) = x^2 entre 0 y 3\n",
    "resultado, error = integrate.quad(lambda x: x**2, 0, 3)\n",
    "print(\"Resultado de la integración:\", resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Otras Librerías para el Procesamiento de Datos\n",
    "Cuando se trabaja con grandes volúmenes de datos o se requieren procesos distribuidos, se pueden utilizar las siguientes librerías:\n",
    "\n",
    "**Dask** \n",
    "- Uso:\n",
    "  \n",
    "Extiende la funcionalidad de Pandas y NumPy para procesar datasets que no caben en la memoria local, ejecutando operaciones de forma distribuida y en paralelo.\n",
    "\n",
    "- Ventaja:\n",
    "\n",
    "Permite manejar grandes volúmenes de datos sin la necesidad de cargar todo el dataset en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Leer un archivo CSV grande en un DataFrame distribuido\n",
    "ddf = dd.read_csv('large_data.csv')\n",
    "\n",
    "# Calcular la media de una columna y forzar la computación\n",
    "media_columna = ddf['columna'].mean().compute()\n",
    "print(\"Media de la columna:\", media_columna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vaex**\n",
    "- Uso:\n",
    "\n",
    "Vaex está diseñado para trabajar con datasets extremadamente grandes (out-of-core), permitiendo operaciones rápidas de filtrado, agregación y transformación sin cargar todo el conjunto de datos en memoria.\n",
    "\n",
    "- Ventaja:\n",
    "\n",
    "Optimiza operaciones de agregación y filtrado en datasets voluminosos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "\n",
    "# Abrir un dataset muy grande almacenado en formato HDF5\n",
    "df = vaex.open('big_data.hdf5')\n",
    "\n",
    "# Calcular la media de una columna de forma muy eficiente\n",
    "media_valor = df.mean('columna')\n",
    "print(\"Media calculada con Vaex:\", media_valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Librerías para Modelado y Machine Learning\n",
    "Estas herramientas proporcionan implementaciones de algoritmos y facilitan el entrenamiento, validación y despliegue de modelos de machine learning, abarcando desde técnicas tradicionales hasta deep learning.\n",
    "\n",
    "### 2.1 Scikit-learn\n",
    "\n",
    "Scikit-learn es la librería más utilizada para algoritmos clásicos de machine learning. Incluye métodos para clasificación, regresión, clustering, reducción de dimensionalidad, entre otros.\n",
    "\n",
    "**Características Principales:** \n",
    "\n",
    "- Modelado y Preprocesamiento: Amplia colección de modelos y herramientas para transformar y escalar datos.\n",
    "- Validación y Selección: Funciones para validación cruzada, búsqueda en cuadrícula de hiperparámetros y evaluación de modelos.\n",
    "- Documentación y Comunidad: Excelente documentación y comunidad activa que facilitan la resolución de dudas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar un clasificador RandomForest\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar el modelo\n",
    "preds = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Librerías de Boosting y Árboles de Decisión\n",
    "Estas herramientas están optimizadas para construir modelos basados en árboles de decisión, que suelen ser muy efectivos en competiciones y aplicaciones prácticas.\n",
    "\n",
    "### XGBoost\n",
    "- Descripción:\n",
    "\n",
    "Implementa gradient boosting de manera altamente optimizada, ofreciendo paralelismo y eficiencia en el entrenamiento.\n",
    "\n",
    "- Ventaja:\n",
    "\n",
    "Frecuentemente ganador en competiciones de machine learning gracias a su rendimiento y capacidad de manejar grandes datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convertir datos a DMatrix, estructura optimizada para XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definir parámetros y entrenar el modelo\n",
    "params = {'objective': 'multi:softmax', 'num_class': 3, 'max_depth': 3, 'eta': 0.1}\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "predictions = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "- Descripción:\n",
    "\n",
    "Desarrollado por Microsoft, LightGBM destaca por su rapidez, eficiencia en el uso de memoria y capacidad para trabajar con datasets de alta cardinalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el Dataset para LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5\n",
    "}\n",
    "model = lgb.train(params, train_data, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "- Descripción:\n",
    "\n",
    "Creado por Yandex, CatBoost maneja de forma nativa variables categóricas, evitando un extenso preprocesamiento y reduciendo el riesgo de sobreajuste mediante técnicas internas de regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el dataset Iris\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar un modelo CatBoost\n",
    "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=5, verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Frameworks de Deep Learning\n",
    "Los frameworks de deep learning permiten construir redes neuronales complejas y se utilizan tanto en investigación como en producción.\n",
    "\n",
    "## TensorFlow/Keras\n",
    "\n",
    "**TensorFlow:** \n",
    "\n",
    "Plataforma de código abierto desarrollada por Google para construir y entrenar modelos de deep learning.\n",
    "\n",
    "**Keras:** \n",
    "\n",
    "API de alto nivel que se integra en TensorFlow, facilitando la creación y experimentación de modelos complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Definir un modelo secuencial simple para clasificación\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(4,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Utilizar el dataset Iris para entrenar el modelo\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "model.fit(data.data, data.target, epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "- Descripción:\n",
    "\n",
    "Framework de deep learning desarrollado por Facebook. Su naturaleza dinámica (“define-by-run”) lo hace muy flexible para la investigación y el desarrollo de arquitecturas personalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definir un modelo simple con PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# (Aquí se incluiría el bucle de entrenamiento y evaluación con datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras Herramientas de Deep Learning\n",
    "- **Theano:**\n",
    "\n",
    "Pionera en el cálculo simbólico y la diferenciación automática. Aunque ya no se utiliza activamente, muchas de sus ideas se han incorporado en TensorFlow y PyTorch.\n",
    "\n",
    "- **MXNet:**\n",
    "\n",
    "Framework escalable y de alto rendimiento, adoptado por Amazon y con soporte para múltiples lenguajes.\n",
    "\n",
    "- **CNTK (Microsoft Cognitive Toolkit):**\n",
    "\n",
    "Librería de deep learning de Microsoft que permite entrenar redes neuronales profundas de forma eficiente.\n",
    "\n",
    "- **Fast.ai:**\n",
    "\n",
    "Construido sobre PyTorch, proporciona abstracciones de alto nivel y cursos educativos, facilitando el acceso a técnicas avanzadas de deep learning.\n",
    "\n",
    "- **PyTorch Lightning:**\n",
    "\n",
    "Organiza y simplifica el código de PyTorch, eliminando tareas repetitivas y permitiendo la escalabilidad y modularidad en proyectos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 AutoML (Automatización del Machine Learning)\n",
    "Estas herramientas buscan automatizar la selección, preprocesamiento, y optimización de modelos.\n",
    "\n",
    "## TPOT\n",
    "- Descripción:\n",
    "\n",
    "Utiliza algoritmos genéticos para explorar y optimizar pipelines de machine learning basados en scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(\"TPOT Score:\", tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-sklearn, H2O AutoML y AutoKeras\n",
    "- **Auto-sklearn:**\n",
    "\n",
    "Automatiza la selección de modelos y la optimización de hiperparámetros basándose en la infraestructura de scikit-learn.\n",
    "- **H2O AutoML:**\n",
    "\n",
    "Ofrece un conjunto completo de herramientas para automatizar el proceso de modelado utilizando la plataforma H2O.\n",
    "- **AutoKeras:**\n",
    "\n",
    " Se centra en la automatización del deep learning, permitiendo construir modelos sin una intervención manual extensa.\n",
    " \n",
    "(Cada uno de estos frameworks cuenta con documentación propia y ejemplos específicos que facilitan su integración en pipelines de machine learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Librerías para Optimización y Ajuste de Hiperparámetros\n",
    "El ajuste de hiperparámetros es clave para maximizar el rendimiento de los modelos. Aquí se presentan algunas herramientas de optimización.\n",
    "\n",
    "### Optuna\n",
    "\n",
    "Optuna es una biblioteca flexible y eficiente para la optimización de hiperparámetros mediante técnicas de búsqueda inteligente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    data = load_iris()\n",
    "    score = cross_val_score(clf, data.data, data.target, cv=3).mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt y Ray Tune\n",
    "- **Hyperopt:**\n",
    "\n",
    " Emplea métodos de optimización bayesiana para buscar la combinación óptima de hiperparámetros.\n",
    "- **Ray Tune:**\n",
    "\n",
    " Permite distribuir la búsqueda de hiperparámetros en entornos escalables aprovechando la paralelización.\n",
    " \n",
    "(Ambas herramientas se integran con pipelines de scikit-learn y otros frameworks, facilitando la experimentación a gran escala.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Herramientas para Interpretabilidad y Explicabilidad de Modelos\n",
    "Entender el comportamiento interno de los modelos es crucial, especialmente en entornos de producción o en aplicaciones sensibles.\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "Utiliza la teoría de valores de Shapley para cuantificar la contribución de cada característica en la predicción de un modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# Supongamos que 'model' es un modelo ya entrenado, por ejemplo, un XGBoost\n",
    "explainer = shap.Explainer(model)\n",
    "# Calcular valores SHAP para un conjunto de datos de prueba\n",
    "shap_values = explainer(X_test)\n",
    "# Generar un resumen gráfico\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME y ELI5\n",
    "- **LIME (Local Interpretable Model-agnostic Explanations):**\n",
    "\n",
    " Genera explicaciones locales para predicciones individuales, ayudando a comprender por qué un modelo realiza cierta predicción.\n",
    "- **ELI5:**\n",
    "\n",
    " Proporciona herramientas y visualizaciones que explican el comportamiento y las decisiones de diversos modelos, facilitando la interpretación de modelos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Otras Herramientas Complementarias\n",
    "\n",
    "### Imbalanced-learn\n",
    "\n",
    "Extiende scikit-learn para manejar datasets desequilibrados, proporcionando técnicas de sobremuestreo (over-sampling) y submuestreo (under-sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Supongamos que X_train e y_train son nuestros datos de entrenamiento\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scikit-Optimize (skopt)**\n",
    "\n",
    "Facilita la optimización de funciones y la búsqueda de hiperparámetros, integrándose de forma natural con scikit-learn.\n",
    "\n",
    "- **H2O.ai**\n",
    "\n",
    "Plataforma completa que, además de ofrecer AutoML, integra algoritmos de machine learning escalables y una interfaz en  para facilitar la implementación y despliegue de modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Librerías para Visualización de Datos\n",
    "La visualización es esencial para el análisis exploratorio, la comunicación de resultados y la interpretación de modelos. Aquí se exponen diversas librerías, desde las más básicas hasta herramientas interactivas y especializadas.\n",
    "\n",
    "### 3.1 Matplotlib\n",
    "\n",
    "Matplotlib es la librería básica y más utilizada para crear gráficos estáticos, animados e interactivos en python.\n",
    "\n",
    "- Características Principales:\n",
    "\n",
    "Gran capacidad de personalización.\n",
    "Base sobre la que se han desarrollado otras librerías de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Crear datos para graficar\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Graficar la función seno\n",
    "plt.plot(x, y, label='sin(x)')\n",
    "plt.title(\"Gráfico del Seno de x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sin(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Seaborn\n",
    "\n",
    "Construida sobre Matplotlib, Seaborn simplifica la creación de gráficos estadísticos y ofrece una estética visual moderna y atractiva.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear un DataFrame de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(100),\n",
    "    'y': np.random.randn(100)\n",
    "})\n",
    "\n",
    "# Graficar un scatter plot utilizando Seaborn\n",
    "sns.scatterplot(x='x', y='y', data=df)\n",
    "plt.title(\"Scatter Plot con Seaborn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Plotly\n",
    "\n",
    "Plotly permite la creación de visualizaciones interactivas y dashboards, ideales para aplicaciones web y análisis exploratorio interactivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Usar un dataset de ejemplo incluido en Plotly\n",
    "df = px.data.iris()\n",
    "\n",
    "# Crear un scatter plot interactivo\n",
    "fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\",\n",
    "                 title=\"Scatter Plot interactivo con Plotly\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Otras Herramientas de Visualización\n",
    "- **Bokeh**\n",
    "\n",
    "Bokeh está diseñada para crear visualizaciones web interactivas y dashboards de alto rendimiento, con soporte para grandes volúmenes de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()  # Para visualizar en un entorno Jupyter Notebook\n",
    "\n",
    "# Crear una figura interactiva\n",
    "p = figure(title=\"Ejemplo Bokeh\", x_axis_label='x', y_axis_label='y')\n",
    "p.line([1, 2, 3, 4, 5], [6, 7, 2, 4, 5], legend_label=\"Línea\", line_width=2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Altair**\n",
    "\n",
    "Basada en la \"Grammar of Graphics\", Altair permite crear visualizaciones declarativas y elegantes de forma concisa, con integración sencilla a Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Crear un DataFrame simple\n",
    "df = pd.DataFrame({'x': range(10), 'y': [x**2 for x in range(10)]})\n",
    "\n",
    "# Crear un gráfico de líneas declarativo\n",
    "chart = alt.Chart(df).mark_line().encode(\n",
    "    x='x',\n",
    "    y='y'\n",
    ")\n",
    "chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geopandas y Folium\n",
    "\n",
    "Estas librerías están especializadas en la visualización de datos geoespaciales y mapas interactivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Crear un mapa centrado en Madrid (latitud, longitud)\n",
    "m = folium.Map(location=[40.4168, -3.7038], zoom_start=6)\n",
    "\n",
    "# Agregar un marcador para Madrid\n",
    "folium.Marker([40.4168, -3.7038], popup=\"Madrid\").add_to(m)\n",
    "\n",
    "# Guardar el mapa en un archivo HTML\n",
    "m.save(\"mapa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Librerías y Herramientas en Áreas Especializadas\n",
    "### 4.1 Procesamiento del Lenguaje Natural (NLP)\n",
    "### NLTK\n",
    "\n",
    "NLTK es un conjunto de herramientas y recursos para el procesamiento y análisis de lenguaje natural, muy útil para tareas de tokenización, análisis sintáctico y más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Descargar recursos necesarios\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Este es un ejemplo de tokenización en NLTK.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "\n",
    "spaCy ofrece un procesamiento rápido y robusto para tareas de NLP, con modelos preentrenados para múltiples idiomas y funciones de reconocimiento de entidades, análisis sintáctico, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar un modelo preentrenado en inglés\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is an example sentence for spaCy.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Gensim está especializada en modelado de tópicos y la creación de representaciones vectoriales de textos, facilitando técnicas como LDA y Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "documents = [\"Este es el primer documento.\", \"Este es el segundo documento.\"]\n",
    "# Tokenizar los documentos\n",
    "texts = [doc.lower().split() for doc in documents]\n",
    "# Crear un diccionario\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# Convertir documentos a formato bolsa de palabras\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# Entrenar un modelo LDA\n",
    "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic:\", idx, \"\\nWords:\", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Aprendizaje por Refuerzo\n",
    "### OpenAI Gym\n",
    "\n",
    "OpenAI Gym proporciona una colección de entornos para desarrollar, probar y comparar algoritmos de aprendizaje por refuerzo (RL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Crear el entorno CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation = env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    # Seleccionar una acción de forma aleatoria\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Baselines3\n",
    "\n",
    "Stable Baselines3 ofrece implementaciones en PyTorch de varios algoritmos de aprendizaje por refuerzo, facilitando la experimentación y el entrenamiento de agentes en distintos entornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Aceleración y Computación en GPU\n",
    "### CuPy\n",
    "\n",
    "CuPy es una librería similar a NumPy, pero diseñada para aprovechar la potencia de las GPUs (especialmente con hardware NVIDIA), acelerando los cálculos numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Crear un array en la GPU\n",
    "x_gpu = cp.arange(10)\n",
    "# Operación en GPU: multiplicar por 2\n",
    "y_gpu = x_gpu * 2\n",
    "# Convertir el resultado a un array de NumPy para visualizarlo\n",
    "print(\"Resultado en CPU:\", cp.asnumpy(y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gestión y Seguimiento de Experimentos\n",
    "### MLflow\n",
    "\n",
    "MLflow es una plataforma integral para gestionar el ciclo de vida completo de proyectos de machine learning, que incluye el seguimiento de experimentos, registro de modelos y despliegue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar datos y entrenar un modelo\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Registrar el modelo y una métrica de desempeño\n",
    "mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "accuracy = model.score(X_test, y_test)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "print(\"Accuracy registrada:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sacred\n",
    "\n",
    "Sacred es una herramienta que facilita la configuración, el seguimiento y la reproducibilidad de experimentos, permitiendo registrar configuraciones, parámetros y resultados de manera sistemática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import Experiment\n",
    "\n",
    "ex = Experiment(\"mi_experimento\")\n",
    "\n",
    "@ex.config\n",
    "def my_config():\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "\n",
    "@ex.automain\n",
    "def main(learning_rate, epochs):\n",
    "    print(f\"Entrenando con tasa de aprendizaje {learning_rate} durante {epochs} épocas\")\n",
    "    # Aquí iría el código de entrenamiento, registrando resultados y métricas según sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelos de Aprendizaje Supervisado\n",
    "\n",
    "Los modelos supervisados se entrenan utilizando datos etiquetados. Se dividen principalmente en dos grandes grupos: modelos de **regresión** (para problemas continuos) y modelos de **clasificación** (para problemas discretos).\n",
    "\n",
    "### 1.1 Modelos de Regresión\n",
    "\n",
    "Estos modelos predicen valores numéricos continuos.\n",
    "\n",
    "#### 1.1.1 Regresión Lineal\n",
    "\n",
    "**Descripción:**  \n",
    "La regresión lineal busca ajustar una línea (o hiperplano en espacios multidimensionales) a los datos, minimizando la suma de los errores al cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo: relación lineal con ruido\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Ajustar el modelo\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "y_pred = lin_reg.predict(X)\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X, y, color='blue', label='Datos')\n",
    "plt.plot(X, y_pred, color='red', label='Regresión Lineal')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regresión Lineal\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coeficiente:\", lin_reg.coef_)\n",
    "print(\"Intersección:\", lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Regresión Polinómica\n",
    "\n",
    "Extiende la regresión lineal para capturar relaciones no lineales transformando las variables de entrada en potencias (por ejemplo, \n",
    "𝑥\n",
    ",\n",
    "𝑥\n",
    "2\n",
    ",\n",
    "𝑥\n",
    "3\n",
    ",\n",
    "…\n",
    "x,x \n",
    "2\n",
    " ,x \n",
    "3\n",
    " ,…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Crear un pipeline para transformación polinómica y regresión lineal\n",
    "polynomial_reg = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lin_reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "polynomial_reg.fit(X, y)\n",
    "y_poly_pred = polynomial_reg.predict(X)\n",
    "\n",
    "# Visualización\n",
    "plt.scatter(X, y, color='blue', label='Datos')\n",
    "plt.plot(X, y_poly_pred, color='green', label='Regresión Polinómica')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Regresión Polinómica\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Regularización: Ridge, Lasso y ElasticNet\n",
    "\n",
    "Estos métodos agregan un término de penalización al error para evitar el sobreajuste:\n",
    "\n",
    "- **Ridge:** \n",
    "\n",
    "Penaliza el cuadrado de los coeficientes.\n",
    "- **Lasso:** \n",
    "\n",
    "Penaliza el valor absoluto, lo que puede reducir algunos coeficientes a cero.\n",
    "- **ElasticNet:** \n",
    "\n",
    "Combina ambas penalizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo \"lasso\"\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "print(\"Coeficientes Lasso:\", lasso_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Modelos de Clasificación\n",
    "Estos modelos asignan etiquetas o clases a los datos. Entre los más comunes se encuentran:\n",
    "\n",
    "### 1.2.1 Regresión Logística\n",
    "\n",
    "A pesar de su nombre, se utiliza para clasificación. Modela la probabilidad de pertenencia a una clase mediante la función sigmoide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Usando el dataset Iris para un problema de clasificación binaria\n",
    "iris = load_iris()\n",
    "X = iris.data[iris.target != 2]  # Usar solo dos clases\n",
    "y = iris.target[iris.target != 2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(\"Exactitud de Regresión Logística:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 k-Nearest Neighbors (kNN)\n",
    "\n",
    "Clasifica una nueva instancia basándose en la mayoría de las etiquetas entre sus \"k\" vecinos más cercanos en el espacio de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(\"Exactitud de kNN:\", accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Support Vector Machines (SVM)\n",
    "\n",
    "SVM busca el hiperplano que mejor separa las clases maximizando el margen entre ellas. Puede usar núcleos (kernels) para separar datos no linealmente separables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"Exactitud de SVM:\", accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Árboles de Decisión y Ensambles\n",
    "### Árboles de Decisión\n",
    "Construyen modelos en forma de árbol donde cada nodo representa una decisión basada en una característica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred_tree = dtree.predict(X_test)\n",
    "print(\"Exactitud de Árbol de Decisión:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "# Visualización del árbol\n",
    "plt.figure(figsize=(12,8))\n",
    "tree.plot_tree(dtree, filled=True, feature_names=iris.feature_names[:4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Es un ensamble de árboles de decisión que mejora la generalización al promediar múltiples modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Exactitud de Random Forest:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (ej.: XGBoost, LightGBM, CatBoost)\n",
    "Construyen modelos de manera secuencial, corrigiendo los errores de modelos anteriores para mejorar la precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Naive Bayes\n",
    "\n",
    "Basado en el teorema de Bayes, asume la independencia entre las características. Es rápido y efectivo en muchos problemas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(\"Exactitud de Naive Bayes:\", accuracy_score(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modelos para Series Temporales\n",
    "Aunque muchos modelos de series temporales pueden clasificarse como modelos de regresión, existen técnicas específicas:\n",
    "\n",
    "### 1.3.1 ARIMA (AutoRegressive Integrated Moving Average)\n",
    "\n",
    "Modela series temporales considerando componentes autorregresivos, de media móvil e integración (para eliminar la no estacionariedad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Simular una serie temporal simple\n",
    "np.random.seed(42)\n",
    "data = np.cumsum(np.random.randn(100))  # Caminata aleatoria\n",
    "serie = pd.Series(data)\n",
    "\n",
    "# Ajustar un modelo ARIMA(1,1,1)\n",
    "model = ARIMA(serie, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Predicción\n",
    "pred = model_fit.forecast(steps=10)\n",
    "plt.plot(serie, label='Serie Original')\n",
    "plt.plot(range(len(serie), len(serie)+10), pred, label='Predicción', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelos de Aprendizaje No Supervisado\n",
    "En este grupo se encuentran los algoritmos que descubren patrones o estructuras en datos sin etiquetas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clustering\n",
    "### 2.1.1 k-Means\n",
    "\n",
    "Algoritmo que agrupa los datos en k clusters, asignando cada punto al centroide más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos simulados\n",
    "X_cluster = np.vstack([np.random.randn(100, 2) + np.array([i*5, i*5]) for i in range(3)])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "plt.scatter(X_cluster[:,0], X_cluster[:,1], c=y_kmeans, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=200, c='red', marker='X')\n",
    "plt.title(\"Clustering con k-Means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Clustering Jerárquico\n",
    "\n",
    "Construye una jerarquía de clusters, ya sea de manera aglomerativa (fusiona clusters) o divisiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (Aglomerativo):\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=3)\n",
    "y_agglo = agglo.fit_predict(X_cluster)\n",
    "\n",
    "plt.scatter(X_cluster[:,0], X_cluster[:,1], c=y_agglo, cmap='rainbow')\n",
    "plt.title(\"Clustering Aglomerativo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 DBSCAN (Density-Based Spatial Clustering)\n",
    "\n",
    "Agrupa puntos densamente conectados y marca como ruido aquellos que se encuentran en áreas de baja densidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "y_dbscan = dbscan.fit_predict(X_cluster)\n",
    "\n",
    "plt.scatter(X_cluster[:,0], X_cluster[:,1], c=y_dbscan, cmap='plasma')\n",
    "plt.title(\"Clustering con DBSCAN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Modelos de Mezcla Gaussiana (GMM)\n",
    "\n",
    "Utiliza modelos probabilísticos para agrupar datos, asumiendo que cada cluster se distribuye según una distribución gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(X_cluster)\n",
    "y_gmm = gmm.predict(X_cluster)\n",
    "\n",
    "plt.scatter(X_cluster[:,0], X_cluster[:,1], c=y_gmm, cmap='coolwarm')\n",
    "plt.title(\"Clustering con GMM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reducción de Dimensionalidad\n",
    "Estos métodos buscan representar los datos en un espacio de menor dimensión, manteniendo la mayor parte de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 PCA (Análisis de Componentes Principales)\n",
    "\n",
    "Transforma las variables originales en un nuevo conjunto de variables (componentes) que maximizan la varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y_kmeans, cmap='viridis')\n",
    "plt.title(\"PCA - Reducción a 2 dimensiones\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 t-SNE y UMAP\n",
    "\n",
    "Métodos no lineales que conservan la estructura local (y global en menor medida) para visualizar datos complejos en 2D o 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (t-SNE):\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_cluster)\n",
    "\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_kmeans, cmap='viridis')\n",
    "plt.title(\"t-SNE - Visualización en 2D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Autoencoders\n",
    "\n",
    "Redes neuronales diseñadas para aprender una representación comprimida (codificación) de los datos de entrada de forma no supervisada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (usando Keras):\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Datos de ejemplo: utilizar los mismos X_cluster o cualquier dataset\n",
    "input_dim = X_cluster.shape[1]\n",
    "encoding_dim = 2  # dimensión comprimida\n",
    "\n",
    "# Definir la arquitectura del autoencoder\n",
    "input_data = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "decoded = Dense(input_dim, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_data, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenar el autoencoder\n",
    "autoencoder.fit(X_cluster, X_cluster, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "# Obtener la codificación\n",
    "encoder = Model(input_data, encoded)\n",
    "X_encoded = encoder.predict(X_cluster)\n",
    "plt.scatter(X_encoded[:,0], X_encoded[:,1], c=y_kmeans, cmap='viridis')\n",
    "plt.title(\"Autoencoder - Representación en 2D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reglas de Asociación\n",
    "\n",
    "Métodos para descubrir relaciones interesantes (reglas de asociación) entre variables en grandes bases de datos, muy utilizados en análisis de cesta de la compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (usando mlxtend):\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Ejemplo: dataset de transacciones (cada fila es una transacción con productos comprados)\n",
    "dataset = pd.DataFrame({\n",
    "    'Leche': [1, 0, 1, 1, 0],\n",
    "    'Pan': [1, 1, 1, 0, 1],\n",
    "    'Mantequilla': [0, 1, 1, 0, 0]\n",
    "})\n",
    "\n",
    "# Calcular conjuntos frecuentes\n",
    "frequent_itemsets = apriori(dataset, min_support=0.6, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelos de Aprendizaje por Refuerzo\n",
    "En el aprendizaje por refuerzo, un agente aprende a tomar decisiones en un entorno mediante recompensas. Los modelos más conocidos incluyen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Q-Learning y SARSA (Métodos Tabulares)\n",
    "\n",
    "- **Q-Learning:**\n",
    "\n",
    "Método fuera de la política que actualiza la función Q de manera iterativa para aprender la política óptima.\n",
    "- **SARSA:**\n",
    "\n",
    "Actualiza la función Q usando la acción realmente tomada, siendo un método en la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Conceptual (Pseudo-código):\n",
    "\n",
    "# Inicializar Q(s, a) arbitrariamente\n",
    "for episodio in range(num_episodios):\n",
    "    s = entorno.reset()\n",
    "    a = seleccionar_accion(s)\n",
    "    for paso in range(max_pasos):\n",
    "        s_next, r, done, info = entorno.step(a)\n",
    "        a_next = seleccionar_accion(s_next)\n",
    "        # Actualizar Q usando la fórmula de SARSA\n",
    "        Q[s, a] = Q[s, a] + alfa * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
    "        s, a = s_next, a_next\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deep Q-Networks (DQN)\n",
    "\n",
    "Extienden Q-Learning usando redes neuronales para aproximar la función Q en entornos con espacios de estados grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (usando Stable Baselines3):\n",
    "\n",
    "python\n",
    "Copiar\n",
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Métodos de Política: Policy Gradient, Actor-Critic, PPO\n",
    "\n",
    "Estos métodos optimizan directamente la política (la función que determina la acción) en lugar de la función de valor.\n",
    "\n",
    "- **Policy Gradient (REINFORCE):**\n",
    "\n",
    "Actualiza la política basada en la recompensa obtenida.\n",
    "- **Actor-Critic:**\n",
    "\n",
    "Combina un actor (política) y un crítico (función de valor) para reducir la varianza en la actualización.\n",
    "- **PPO (Proximal Policy Optimization):**\n",
    "\n",
    " Método avanzado y robusto que mejora la estabilidad del entrenamiento.\n",
    " \n",
    "(Debido a la complejidad, se recomienda revisar implementaciones específicas en librerías como Stable Baselines3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelos de Deep Learning\n",
    "Los modelos de deep learning utilizan redes neuronales profundas para aprender representaciones complejas y se aplican en una gran variedad de tareas.\n",
    "\n",
    "## 4.1 Redes Neuronales Artificiales (Multi-layer Perceptron, MLP)\n",
    "\n",
    "Es el modelo básico de red neuronal feedforward, compuesto por capas densamente conectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (con Keras):\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Suponiendo que X y y son nuestros datos de entrada y etiquetas\n",
    "# model.fit(X, y, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Diseñadas para procesar datos con estructura de rejilla (por ejemplo, imágenes). Utilizan capas convolucionales para extraer características espaciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model_cnn = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model_cnn.fit(X_imagenes, y_etiquetas, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Redes Neuronales Recurrentes (RNN), LSTM y GRU\n",
    "\n",
    "Ideales para datos secuenciales, como series temporales o procesamiento del lenguaje.\n",
    "\n",
    "- **RNN:**\n",
    "\n",
    "Modelo básico que procesa secuencias.\n",
    "- **LSTM/GRU:** \n",
    "\n",
    "Variantes que solucionan el problema del desvanecimiento del gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Práctico (LSTM con Keras):\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64),\n",
    "    LSTM(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model_lstm.fit(secuencias, etiquetas, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Modelos Basados en Atención y Transformers\n",
    "\n",
    "Utilizan mecanismos de atención para procesar secuencias sin recurrencia. Son la base de modelos de lenguaje modernos (por ejemplo, BERT, GPT).\n",
    "\n",
    "(Debido a su complejidad, se recomienda explorar librerías como Hugging Face Transformers para implementaciones completas.)\n",
    "\n",
    "### 4.5 Redes Generativas: GANs y Autoencoders\n",
    "- **Generative Adversarial Networks (GANs):**\n",
    "\n",
    "Consisten en dos redes (generador y discriminador) que se entrenan de manera competitiva para generar datos realistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo Básico Conceptual:\n",
    "\n",
    "# Se definen dos modelos: el generador y el discriminador.\n",
    "# Durante el entrenamiento, el generador intenta producir datos que el discriminador no pueda distinguir de los reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temas Adicionales en Machine Learning\n",
    "\n",
    "Se incluyen temas relacionados con el despliegue, la ingeniería de datos, la integración en la nube, técnicas emergentes y casos de uso específicos.\n",
    "\n",
    "\n",
    "## 1. Despliegue y Producción de Modelos (MLOps)\n",
    "\n",
    "**Concepto:**  \n",
    "El despliegue y la producción de modelos (MLOps) comprenden las prácticas, herramientas y procesos necesarios para llevar un modelo de machine learning desde el desarrollo hasta un entorno de producción de manera escalable y eficiente.\n",
    "\n",
    "**Aspectos Clave:**\n",
    "- **Frameworks Web:** Uso de Flask, Django o FastAPI para crear APIs que sirvan los modelos.\n",
    "- **Servidores de Modelos:** Herramientas como TensorFlow Serving y ONNX Runtime que permiten desplegar modelos de forma optimizada.\n",
    "- **Orquestación y Automatización:** Plataformas como Kubeflow y MLflow facilitan la integración, monitorización y mantenimiento de modelos en producción.\n",
    "\n",
    "\n",
    "## 2. Ingeniería de Datos y Pipelines de Datos\n",
    "\n",
    "**Concepto:**  \n",
    "Se enfoca en la preparación, transformación y orquestación de datos, asegurando que los modelos reciban información limpia y estructurada.\n",
    "\n",
    "**Aspectos Clave:**\n",
    "- **Procesos ETL (Extract, Transform, Load):** Procedimientos para extraer datos de diversas fuentes, transformarlos y cargarlos en sistemas de almacenamiento.\n",
    "- **Orquestación de Tareas:** Herramientas como Apache Airflow y Luigi permiten planificar y coordinar flujos de trabajo complejos.\n",
    "- **Integración de Datos:** Conexión con bases de datos relacionales y NoSQL para manejar la ingesta y procesamiento a gran escala.\n",
    "\n",
    "\n",
    "## 3. Integración con Plataformas en la Nube\n",
    "\n",
    "**Concepto:**  \n",
    "Utilizar servicios en la nube para escalar el entrenamiento, almacenamiento y despliegue de modelos, facilitando la operación y el mantenimiento en entornos productivos.\n",
    "\n",
    "**Proveedores y Servicios:**\n",
    "- **AWS:** Servicios como Amazon SageMaker, EC2 y S3.\n",
    "- **Google Cloud:** AI Platform, BigQuery y Compute Engine.\n",
    "- **Azure:** Azure Machine Learning y Data Factory.\n",
    "\n",
    "\n",
    "## 4. Técnicas Emergentes\n",
    "\n",
    "**Concepto:**  \n",
    "Nuevos enfoques y metodologías que están ganando tracción en la comunidad de machine learning y que potencian la capacidad de los modelos para adaptarse y aprender de manera más eficiente.\n",
    "\n",
    "**Ejemplos:**\n",
    "- **Aprendizaje Federado:** Permite entrenar modelos de forma distribuida sin centralizar los datos, preservando la privacidad.\n",
    "- **Meta-learning:** Técnicas que permiten a los modelos \"aprender a aprender\", facilitando la adaptación a nuevas tareas con pocos datos.\n",
    "- **Auto-supervisión:** Métodos que explotan los datos no etiquetados para extraer información útil y mejorar el aprendizaje del modelo.\n",
    "\n",
    "\n",
    "## 5. Casos de Uso Específicos\n",
    "\n",
    "**Concepto:**  \n",
    "Aplicaciones concretas de machine learning en diferentes dominios que requieren técnicas especializadas.\n",
    "\n",
    "**Ejemplos:**\n",
    "- **Visión por Computadora:** Uso de CNNs y técnicas de deep learning para tareas como reconocimiento de imágenes, detección de objetos y segmentación.\n",
    "- **Procesamiento del Lenguaje Natural Avanzado:** Modelos basados en Transformers (por ejemplo, BERT y GPT) para comprender y generar lenguaje natural con alta precisión.\n",
    "- **Análisis de Series Temporales:** Técnicas como ARIMA, modelos de estado y redes neuronales recurrentes para predecir comportamientos futuros basados en datos históricos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
