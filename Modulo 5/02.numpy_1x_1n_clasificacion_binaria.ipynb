{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación binaria: 1 neurona con 1 sola columna en X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Datos de ejemplo (ficticios)\n",
    "# Podrían ser pesos en kg, y = 0 o 1 según alguna condición\n",
    "X = np.array([30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95], dtype=float)\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definir una neurona\n",
    "\n",
    "**Función de activación**: sigmoide\n",
    "\n",
    "La neurona hace la siguiente operación:\n",
    "\n",
    "$$\n",
    "z^{(i)} = w \\cdot x^{(i)} + b\n",
    "$$\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\n",
    "$$\n",
    "\n",
    "donde $\\sigma$ es la **función sigmoide**.\n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "Vamos a definir una función `forward` que recibe:\n",
    "\n",
    "- `X`: vector (N,) de ejemplos, cada valor es un “peso” (o característica)  \n",
    "- `w`: un escalar (el peso de la neurona)  \n",
    "- `b`: un escalar (el sesgo)\n",
    "\n",
    "Devuelve el valor de la **neurona** $\\hat{y}$ para cada ejemplo, aplicando sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward(X, w, b):\n",
    "    \"\"\"\n",
    "    Aplica la neurona con sigmoide a todos los datos X.\n",
    "    X: shape (N,)\n",
    "    w, b: escalares\n",
    "    Return: y_pred (N,) con valores entre 0 y 1\n",
    "    \"\"\"\n",
    "    z = w * X + b            # z^{(i)} = w * x^{(i)} + b\n",
    "    y_pred = sigmoid(z)      # Aplicamos la sigmoide\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Función de costo: Entropía Cruzada Binaria (BCE)\n",
    "\n",
    "Para clasificación binaria, se suele usar la **binary cross entropy** (logistic loss).\n",
    "\n",
    "La Binary Cross Entropy (BCE), también conocida como Log Loss, mide la diferencia entre las etiquetas reales y las probabilidades predichas por el modelo.\n",
    "\n",
    "$$\n",
    "\\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^N \\Bigl[y^{(i)}\\log\\bigl(\\hat{y}^{(i)}\\bigr) + \\bigl(1 - y^{(i)}\\bigr)\\log\\bigl(1 - \\hat{y}^{(i)}\\bigr)\\Bigr].\n",
    "$$\n",
    "\n",
    "\n",
    "> En muchos casos, se aplica `np.clip` para evitar un log(0), lo que produce `-inf`.\n",
    "\n",
    "También se llama Log Loss porque utiliza el logaritmo de las probabilidades y penaliza mucho más las predicciones incorrectas cuando el modelo tiene alta confianza en una clase equivocada.\n",
    "\n",
    "¿Por qué usar BCE y no accuracy o f1_score? Las métricas como accuracy o F1-score se usan para evaluar el rendimiento del modelo, pero no para optimizarlo. La razón es que estas métricas no tienen propiedades matemáticas que permitan calcular gradientes de manera efectiva para la optimización, es decir, no son diferenciables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: (N,) - Valores en [0, 1] predichos por la sigmoide\n",
    "    y_true: (N,) - 0 o 1 (etiquetas reales)\n",
    "    \"\"\"\n",
    "    # Para evitar problemas de log(0), se acostumbra a añadir un pequeño epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradientes para w y b\n",
    "\n",
    "### 4.1 Derivada parcial de la BCE respecto a w\n",
    "\n",
    "Recordando que:\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\sigma(z^{(i)}), \\quad z^{(i)} = w x^{(i)} + b,\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z} \\text{BCE} = \\hat{y}^{(i)} - y^{(i)},\n",
    "$$\n",
    "(cuando se trata de la entropía cruzada con sigmoide)\n",
    "\n",
    "entonces:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w} \\text{BCE} \n",
    "= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\frac{\\partial z^{(i)}}{\\partial w}\n",
    "= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot x^{(i)}.\n",
    "$$\n",
    "\n",
    "### 4.2 Derivada parcial de la BCE respecto a b\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} \\text{BCE}\n",
    "= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot \\frac{\\partial z^{(i)}}{\\partial b}\n",
    "= \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}^{(i)} - y^{(i)}) \\cdot 1,\n",
    "$$\n",
    "porque $\\frac{\\partial}{\\partial b}(b)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_w(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de la BCE wrt w (escalares).\n",
    "    \"\"\"\n",
    "    y_pred = forward(X, w, b)            # (N,)\n",
    "    # (y_pred - y): (N,)\n",
    "    # Multiplicamos elemento a elemento por X -> (N,)\n",
    "    # y hacemos la media\n",
    "    return np.mean((y_pred - y) * X)\n",
    "\n",
    "def grad_b(X, y, w, b):\n",
    "    y_pred = forward(X, w, b)\n",
    "    # Solo (y_pred - y), y su media\n",
    "    return np.mean(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento (Descenso por gradiente)\n",
    "\n",
    "Ahora unimos todo en un **bucle de entrenamiento**.\n",
    "\n",
    "- Tras el entrenamiento, la neurona va a **aproximarse** a valores de $ w $ y $ b $ que **maximicen** la verosimilitud de la clasificación (equivalentemente, **minimicen** la entropía cruzada).\n",
    "- `y_pred_final` nos dará probabilidades cercanas a 0 o 1.  \n",
    "  - Para tomar una **decisión binaria** final, podríamos hacer `pred_class = (y_pred_final >= 0.5).astype(int)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/5000, Loss: 0.628088, w: 0.1355, b: -0.5517\n",
      "Epoch 800/5000, Loss: 5.873374, w: 0.1659, b: -1.0899\n",
      "Epoch 1200/5000, Loss: 3.089710, w: 0.0257, b: -1.6168\n",
      "Epoch 1600/5000, Loss: 6.039287, w: 0.1994, b: -2.1302\n",
      "Epoch 2000/5000, Loss: 6.116875, w: 0.2153, b: -2.6213\n",
      "Epoch 2400/5000, Loss: 0.361202, w: 0.1126, b: -3.1035\n",
      "Epoch 2800/5000, Loss: 1.343982, w: -0.0137, b: -3.5810\n",
      "Epoch 3200/5000, Loss: 0.803060, w: -0.0135, b: -4.0401\n",
      "Epoch 3600/5000, Loss: 0.856839, w: -0.0023, b: -4.4902\n",
      "Epoch 4000/5000, Loss: 0.531622, w: 0.0138, b: -4.9205\n",
      "Epoch 4400/5000, Loss: 0.526385, w: 0.0233, b: -5.3326\n",
      "Epoch 4800/5000, Loss: 0.565527, w: 0.0312, b: -5.7401\n",
      "\n",
      "Entrenamiento terminado.\n",
      "Parámetros finales: w = 0.0352, b = -5.9428\n",
      "Predicciones finales (probabilidades): [0.00747979 0.00890461 0.01059794 0.01260918 0.01499633 0.01782725\n",
      " 0.02118108 0.02514971 0.02983926 0.03537152 0.0418852  0.04953676\n",
      " 0.05850077 0.06896917]\n",
      "Predicciones (clases): [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Valores reales (clase):                [0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "w = np.random.randn() * 0.02  # un valor aleatorio pequeño\n",
    "b = np.random.randn() * 0.02\n",
    "\n",
    "learning_rate = 0.06\n",
    "num_epochs = 7000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    y_pred = forward(X, w, b)\n",
    "    loss_value = binary_cross_entropy(y_pred, y)\n",
    "    \n",
    "    dw = grad_w(X, y, w, b)\n",
    "    db = grad_b(X, y, w, b)\n",
    "    \n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # (Opcional) imprimir cada cierto número de épocas\n",
    "    if (epoch+1) % 400 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_value:.6f}, w: {w:.4f}, b: {b:.4f}\")\n",
    "\n",
    "# 6. Revisar resultados finales\n",
    "print(\"\\nEntrenamiento terminado.\")\n",
    "print(f\"Parámetros finales: w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "# 7. Predicción final\n",
    "y_pred_final = forward(X, w, b)\n",
    "print(\"Predicciones finales (probabilidades):\", y_pred_final)\n",
    "y_pred_classes = (y_pred_final >= 0.5).astype(int)\n",
    "print(\"Predicciones (clases):\", y_pred_classes)\n",
    "print(\"Valores reales (clase):               \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    # Convertir predicciones a clases binarias (0 o 1) si son probabilidades\n",
    "    y_pred_classes = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calcular el número de aciertos\n",
    "    correct_predictions = np.sum(y_true == y_pred_classes)\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.43%\n"
     ]
    }
   ],
   "source": [
    "accuracy_value = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy: {accuracy_value:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
